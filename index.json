[{"content":" In this blog post we look into Axon Framework is a framework for building evolutionary, event-driven microservice systems, based on the principles of Domain Driven Design, Command-Query Responsibility Segregation (CQRS) and Event Sourcing.\nWe will disable the Axon Server and replaced it with the MongoDB extension this extension provides functionality to leverage Mongo as an EventStorageEngine (to be used by an EventStore) and TokenStore.\nFor more information on anything Axon, please visit our website, Axon Framework.\nCommands A command tells our application to do something Events An event is a notification of something that has happened. Query Queries could be simplified by storing a copy of the data in a form easily In many cases, updating the query models can happen asynchronously from processing the transaction: eventual consistency Create Project Let\u0026rsquo;s create a Maven Spring Boot project with the following dependencies\nactuator webflux hateoas reactive mongodb Run the following commands :\nexport KBOOT_NAME=kboot-axon export KBOOT_APPL_NAME=KbootAxonDemoApplication export KBOOT_APPL_DESC=\u0026#34;Axon Demo Project\u0026#34; export KBOOT_JAVA_VERS=17 http https://start.spring.io/starter.tgz \\ dependencies==actuator,webflux,dependencies=data-mongodb-reactive,hateoas \\ description==$KBOOT_APPL_DESC \\ applicationName==$KBOOT_APPL_NAME \\ name==$KBOOT_NAME \\ groupId==ch.keepcalm \\ artifactId==$KBOOT_NAME \\ packageName==ch.keepcalm.demo \\ javaVersion==$KBOOT_JAVA_VERS \\ language==kotlin \\ baseDir==$KBOOT_NAME| tar -xzvf - cd $KBOOT_NAME http https://raw.githubusercontent.com/marzelwidmer/marzelwidmer.github.io/master/img/banner.txt \\ \u0026gt; src/main/resources/banner.txt rm src/main/resources/application.properties echo \u0026#34;spring: application: name: $KBOOT_NAME\u0026#34; | \u0026gt; src/main/resources/application.yaml idea . Let\u0026rsquo;s modify the pom.xml to handle HATEOAS with WebFlux. Search the spring-boot-starter-hateoas dependency and exclude the spring-boot-starter-web\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-hateoas\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; ","permalink":"https://blog.marcelwidmer.org/blog/2022/2022-24-09-axon-mongodb-extention/","summary":"AxonIQ - Spring Boot - MongoDB extension - CQRS","title":"Axon Framework with MongoDB extension"},{"content":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.\nMinikube Setup Monitoring - Grafana-Prometheus Install Argo CD Create App on Argo CD Minikube Setup Local Development Environment with minikube.\nBrew Installation Packages brew install hyperkit brew install minikube brew install helm brew install httpie brew install stern brew install argocd brew install sops OSX Minikube Test Cluster Hyperkit Setup first minikube hyperkit driver.\nminikube config set driver hyperkit Minikube Addons Configure some addons for the test cluster.\nminikube addons enable dashboard -p test minikube addons enable metrics-server -p test minikube addons enable ingress -p test minikube addons enable registry -p test Start Minikube Now start the minikube test cluster with some memory and cpu settings.\nminikube start --memory 6144 --cpus 4 -p test Install Monitoring Let\u0026rsquo;s install Prometheus and Grafana with Helm. Add Repository and install it on the monitoring namespace.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install --namespace monitoring --create-namespace prometheus prometheus-community/kube-prometheus-stack Forward Grafana Search for the Pod.\nkubectl get pod -n monitoring | grep prometheus-grafana prometheus-grafana-bd89cc787-snlk6 2/2 Running 0 9m34s Forward it port http://localhost:3000 log in with the admin username and prom-operator password and you‚Äôll see a lot of ready for user graphs\nkubectl -n monitoring port-forward prometheus-grafana-bd89cc787-snlk6 3000:3000 Install ARGO CD Create a separate namespace argocd to install argo.\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Port Forwarding to ARGO Server kubectl port-forward svc/argocd-server -n argocd 8080:443 Change Argo Admin Password The default Password is the Argo Server name.\nkubectl get pod -n argocd | grep argocd-server argocd-server-5bc896856-xvt92 1/1 Running 0 71m Use the Argo CLI to login with user admin and change the password.\nargocd login localhost:8080 argocd account update-password Create App with directory-recurse argocd app create kboot --repo https://github.com/marzelwidmer/argo-demo.git \\ --path manifest \\ --directory-recurse \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default Synch App Argo will be sync every 5 min. You can force it via AdminUI https://localhost:8080 or with the following command :\nargocd app sync kboot Test Contracts inside K8s kubectl run -i --rm --restart=Never curl-client \\ --image=tutum/curl:alpine \\ --command -- curl -s \u0026#39;http://contracts/api/persons\u0026#39; -H \u0026#39;Content-Type: application/json\u0026#39; -w \u0026#34;\\n\u0026#34; [{\u0026#34;firstName\u0026#34;:\u0026#34;John\u0026#34;}] pod \u0026#34;curl-client\u0026#34; deleted Test Contracts over Ingress You can edit the /etc/hosts file for details please check my other Blog Minikube Ingress Controller\nhttp -j http://test.minikube.me/api/persons HTTP/1.1 200 OK Connection: keep-alive Content-Encoding: gzip Content-Type: application/json Date: Sun, 25 Oct 2020 08:34:33 GMT Matched-Stub-Id: e6546194-eda5-4d86-b644-9694fd421ed6 Server: nginx/1.19.1 Transfer-Encoding: chunked Vary: Accept-Encoding, User-Agent [ { \u0026#34;firstName\u0026#34;: \u0026#34;John\u0026#34; } ] Comming soon\u0026hellip; Manage Secrets with SealedSecrets\nSee: https://github.com/bitnami-labs/sealed-secrets\nSealed Secrets Install Sealed Secrets with Helm to manage Secrets. see: https://hub.kubeapps.com/charts/stable/sealed-secrets\nhelm install sealed --namespace kube-system stable/sealed-secrets Install client-side tool into /usr/local/bin/ GOOS=$(go env GOOS) GOARCH=$(go env GOARCH) wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.12.4/kubeseal-$GOOS-$GOARCH sudo install -m 755 kubeseal-$GOOS-$GOARCH /usr/local/bin/kubeseal Create a sealed secret file for Redis kubectl create secret generic redis --dry-run=client --from-literal=database-password=verySecurePassword -o yaml | \\ kubeseal \\ --controller-name=sealed-sealed-secrets \\ --controller-namespace=kube-system \\ --format yaml \u0026gt; sealed-redis-secret.yaml Apply the sealed secret kubectl create -f sealed-redis-secret.yaml Get Secret Running kubectl get secret redis -o yaml will show the decrypted secret that was generated from the sealed secret. Both the SealedSecret and generated Secret must have the same name and namespace.\nkubectl get secret redis -o yaml References:\nBlog Minikube Ingress Controller ingress-minikube\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-10-05-gitops-argo/","summary":"","title":"GitOps with Argo CD"},{"content":"Harbor is an open source trusted cloud native registry project that stores, signs, and scans content. Harbor extends the open source Docker Distribution by adding the functionalities usually required by users such as security, identity and management. Having a registry closer to the build and run environment can improve the image transfer efficiency. Harbor supports replication of images between registries, and also offers advanced security features such as user management, access control and activity auditing.\nHarbor is hosted by the Cloud Native Computing Foundation (CNCF). If you are an organization that wants to help shape the evolution of cloud native technologies, consider joining the CNCF. For details about who\u0026rsquo;s involved and how Harbor plays a role, read the CNCF announcement.\nHarbor GitHub Melvin medium.com\nhelm repo add harbor https://helm.goharbor.io git clone https://github.com/goharbor/harbor-helm.git helm install harbor harbor/harbor -n harbor k get ingress Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress NAME CLASS HOSTS ADDRESS PORTS AGE harbor-harbor-ingress \u0026lt;none\u0026gt; core.harbor.domain 192.168.64.6 80, 443 5m34s harbor-harbor-ingress-notary \u0026lt;none\u0026gt; notary.harbor.domain 192.168.64.6 80, 443 5m34s nginx-ingress \u0026lt;none\u0026gt; test.minikube.me 192.168.64.6 80 5m34s sudo vi /etc/hosts 192.168.64.6 core.harbor.domain http://core.harbor.domain username: admin password: Harbor12345 First thing we need to create a new project inside Harbor. This will be where we will be storing all the project related Docker images. Then we move over to create a new user within Harbor. Lastly we need to add the newly created user into our project member. On a terminal, we will log in Harbor repository\ndocker login -udevops core.harbor.domain Password: Error response from daemon: Get https://core.harbor.domain/v2/: x509: certificate signed by unknown authority This is a known issue with Docker private repository. We can resolve this simply by instruct our mac OS to trust the Harbor repository self signed ca cert. First we need to get the Harbor ca cert. This can be downloaded from our Project -\u0026gt; Repositories tag, click on the ‚ÄúRegistry Certificate‚Äù. This will download the ca.crt file. Execute the following command.\nsecurity add-trusted-cert -d -r trustRoot -k ~/Library/Keychains/login.keychain ./ca.crt Restart Docker process after the operation.\ndocker login -udevops core.harbor.domain Password: Login Succeeded We will proceed to tag the Docker image (e.g. openpolicyagent/opa) according to Harbor convention format core.harbor.domain//:\ndocker tag openpolicyagent/opa:latest core.harbor.domain/devops/openpolicyagent/opa:latest We will finally push the image into our Harbor repository\ndocker push core.harbor.domain/devops/openpolicyagent/opa:latest Install cert-manager [https://ruzickap.github.io/k8s-harbor/part-03/#install-cert-manager](install cert-manager)\nCreate cert-manager namespace and label.\nkubectl create namespace cert-manager kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true Add Helm Repository and install it.\nhelm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager \\ --namespace cert-manager \\ --wait jetstack/cert-manager \\ --version v0.10.0 Verifying the installation\nkubectl get pods --namespace cert-manager NAME READY STATUS RESTARTS AGE cert-manager-556549df9-bjr5v 1/1 Running 0 2m2s cert-manager-cainjector-69d7cb5d4-bnngd 1/1 Running 0 2m2s cert-manager-webhook-c5bdf945c-zmwz5 1/1 Running 0 2m2s Install Harbor [https://ruzickap.github.io/k8s-harbor/part-04/#install-harbor-using-helm](Harbor Helm installation)\nCreate harbor-system namespace if not exist already and label namespace.\nkubectl get namespace harbor-system \u0026amp;\u0026gt; /dev/null || kubectl create namespace harbor-system kubectl label namespace --overwrite harbor-system app=kubed ","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-10-05-harbor/","summary":"","title":"Harbor"},{"content":"The sample code can be found on GitHub. 1\nPrecondition Spring Caching with Hazelcast Let\u0026rsquo;s get ready first our Spring Boot application with the following dependencies.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-cache\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Also let\u0026rsquo;s take the hazelcast-all from com.hazelcast that include the k8s dependencies. The version 4.x.x will also support yaml configuration of hazelcast.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.hazelcast\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hazelcast-all\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.0.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Let\u0026rsquo;s implement a real foo service with a super Chuck Norris API for the moment. The API will take a string on /api/[key] eg. /api/foo this will create a UUID and will put it in the cache and give it back as response. The second call with the same key foo will give the same UUID on cache hit.\nLet\u0026rsquo;s enable the caching @EnableCaching Implement the Rest API with the @CacheConfig annotation. @SpringBootApplication @EnableCaching class DemoHazelcastApplication fun main(args: Array\u0026lt;String\u0026gt;) { runApplication\u0026lt;DemoHazelcastApplication\u0026gt;(*args) } @RestController @RequestMapping(value = [\u0026#34;/api\u0026#34;], produces = [MediaType.APPLICATION_JSON_VALUE]) class Controller(private val cache: Cache) { @GetMapping(path = [\u0026#34;/{key}\u0026#34;]) fun getStuff(@PathVariable(\u0026#34;key\u0026#34;) key: String) = cache.getUUID(key) } @CacheConfig(cacheNames = [\u0026#34;map\u0026#34;]) @Service class Cache { @Cacheable(value = [\u0026#34;map\u0026#34;], key = \u0026#34;#key\u0026#34;) fun getUUID(key: String): UUID? = UUID.randomUUID().also { println(\u0026#34;Generated $it\u0026#34;) } } Kustomize Configuration For the kustomize configuration I will only point to the important hazelcast configurations. You can find the configurations on GitHub. 1\nDeployment The important point in the Deployment ist to expose the containerPort: 5701 for the Hazelcast communications (synchronization) between the Pods.\napiVersion: apps/v1 kind: Deployment spec: template: spec: containers: - image: c3smonkey/template name: hazelcast-demo ports: - containerPort: 8080 name: 8080-tcp protocol: TCP - containerPort: 5701 name: 5701-tcp protocol: TCP Service Here also the important point here is the spec.ports.hazelcast part.\napiVersion: v1 kind: Service metadata: name: app spec: type: LoadBalancer selector: app: app ports: - name: 8080-8080 port: 8080 protocol: TCP targetPort: 8080 - name: hazelcast port: 5701 protocol: TCP targetPort: 5701 Hazelcast Configuration The important part here is the hazelcast.network.join.kubernetes.service-dns who point to the internal service address.\nhazelcast: network: join: multicast: enabled: false kubernetes: enabled: true namespace: dev service-dns: hazelcast-demoservice.dev.svc.cluster.local hazelcast-demo\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-10-04-hazelcast-k8s/","summary":"","title":"Kubernetes Hazelcast Distributed Caching"},{"content":"This will demonstrate how we can deal with a Blocking API in a Reactive World.\nThe GitHb 1 sample provides a\nsoap-server who demonstrate the blocking downstream API. flux-client with REST API lockdown that will call the blocking SOAP endpoint and. Blockhound 2 will throw an exception. easing have an implemented from Avoiding Reactor Meltdown 3 show case how to manage Blocking API. With this approach to manage Blocking API in the same service ant not in a separate service we have all the nice features like retry filter map and so on in our Service A from the Reactive Streams API.\nWe also not have to manage a other service who will handle it for us. With this we have less network hops, Organisations-Issues, Deployment etc. who are sometimes increase complexity and so on.\nRest API BlockHound Plugin SOAP Server SOAP with HTTPie Server Implementation Flux Client API Lockdown Switzerland http :8080/api/lockdown/Switzerland API easing Switzerland http :8080/api/easing/Switzerland Blockhound BlockHound is a Java agent to detect blocking calls from non-blocking threads. Add the following or latest dependency from blockhound.\n\u0026lt;!-- Blockhound\t--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.projectreactor.tools\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;blockhound\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.3.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Install the agent BlockHound.install()\nfun main(args: Array\u0026lt;String\u0026gt;) { BlockHound.install() runApplication\u0026lt;KbootFluxWS\u0026gt;(*args) } SOAP Server The Server have an implementation with a demonstration how we can write own Kotlin DSL.\nDSL 4 country { name = \u0026#34;Switzerland\u0026#34; capital = \u0026#34;Bern\u0026#34; population = 8_603_900 currency = \u0026#34;CHF\u0026#34; } WSDL http://localhost:8888/ws/countries.wsdl\nEnd-Point http://localhost:8888/ws\nRequest \u0026lt;soapenv:Envelope xmlns:soapenv=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34; xmlns:gs=\u0026#34;http://keepcalm.ch/web-services\u0026#34;\u0026gt; \u0026lt;soapenv:Header/\u0026gt; \u0026lt;soapenv:Body\u0026gt; \u0026lt;gs:getCountryRequest\u0026gt; \u0026lt;gs:name\u0026gt;Switzerland\u0026lt;/gs:name\u0026gt; \u0026lt;/gs:getCountryRequest\u0026gt; \u0026lt;/soapenv:Body\u0026gt; \u0026lt;/soapenv:Envelope\u0026gt; Call Service with HTTPie printf \u0026#39;\u0026lt;soapenv:Envelope xmlns:soapenv=\u0026#34;http://schemas.xmlsoap.org/soap/envelope/\u0026#34; xmlns:gs=\u0026#34;http://keepcalm.ch/web-services\u0026#34;\u0026gt; \u0026lt;soapenv:Header/\u0026gt; \u0026lt;soapenv:Body\u0026gt; \u0026lt;gs:getCountryRequest\u0026gt; \u0026lt;gs:name\u0026gt;Switzerland\u0026lt;/gs:name\u0026gt; \u0026lt;/gs:getCountryRequest\u0026gt; \u0026lt;/soapenv:Body\u0026gt; \u0026lt;/soapenv:Envelope\u0026gt;\u0026#39;| http --follow --timeout 3600 POST http://localhost:8888/ws \\ Content-Type:\u0026#39;text/xml\u0026#39; Implementation Router Table bean { router { \u0026#34;api\u0026#34;.nest { GET(\u0026#34;/lockdown/{name}\u0026#34;) { val countryService = ref\u0026lt;CountryService\u0026gt;() ok().body(BodyInserters.fromValue( countryService.getCountryByName(it.pathVariable(\u0026#34;name\u0026#34;))) ) } GET(\u0026#34;/easing/{name}\u0026#34;) { val countryReactiveService = ref\u0026lt;CountryReactiveService\u0026gt;() ok().body( BodyInserters.fromPublisher(countryReactiveService.getCountryByName(it.pathVariable(\u0026#34;name\u0026#34;)), GetCountryResponse::class.java) ) } } } } Reactive Service @Service class CountryReactiveService (private val soapClient: SoapClient) { fun getCountryByName(name: String): Mono\u0026lt;GetCountryResponse\u0026gt; { return soapClient.getCountryReactive(name) } } Reactive SOAP Client .subscribeOn(Schedulers.boundedElastic())\nfun getCountryReactive(country: String): Mono\u0026lt;GetCountryResponse\u0026gt; { val request = GetCountryRequest() request.name = country log.info(\u0026#34;Requesting location for $country\u0026#34;) return Mono.fromCallable { webServiceTemplate .marshalSendAndReceive(\u0026#34;http://localhost:8888/ws/countries\u0026#34;, request, SoapActionCallback( \u0026#34;http://keepcalm.ch/web-services/GetCountryRequest\u0026#34;)) as GetCountryResponse } // properly schedule above blocking call on // scheduler meant for blocking tasks .subscribeOn(Schedulers.boundedElastic()) } kboot-flux-meets-soap\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBlockhound\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvoiding Reactor Meltdown\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKotlin DSL in under an hour | Do Super Language with Kotlin\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-05-10-kboot-flux-meets-soap/","summary":"","title":"Kboot Flux Meets Soap"},{"content":"Apache Camel is an open source integration framework that empowers you to quickly and easily integrate various systems consuming or producing data.\nPrecondition on OSX Create Project Check Camel Context File Route FTP Route Choice Route Precondition on OSX We will also use command line ftp commands for this you need the ftp command line tool this can be installed with :\nbrew install inetutils Create Project Run the following commands :\nexport KBOOT_NAME=kboot-camel export KBOOT_APPL_NAME=KbootCamel http https://start.spring.io/starter.tgz \\ dependencies==camel,actuator,webflux \\ description==\u0026#34;Demo project Spring Boot\u0026#34; \\ applicationName==$KBOOT_APPL_NAME \\ name==$KBOOT_NAME \\ groupId==ch.keepcalm \\ artifactId==$KBOOT_NAME \\ packageName==ch.keepcalm.demo \\ javaVersion==11 \\ language==kotlin \\ baseDir==$KBOOT_NAME| tar -xzvf - cd $KBOOT_NAME http https://raw.githubusercontent.com/marzelwidmer/marzelwidmer.github.io/master/img/banner.txt \\ \u0026gt; src/main/resources/banner.txt rm src/main/resources/application.properties echo \u0026#34;spring: application: name: $KBOOT_NAME\u0026#34; | \u0026gt; src/main/resources/application.yaml idea . Change Maven Dependencies When working with Camel best practice is to change the generated Maven pom.xml from the start.spring.io Like in the official description of Apache Camel camel-spring-boot So lets refactor our pom.xml\nWe take the version from the camel-spring-boot-starter move it up to the properties section with the property name camel.version. Add the Camel BOM dependencieManagement section to it.\nProperties Section \u0026lt;properties\u0026gt;` \u0026lt;camel.version\u0026gt;3.2.0\u0026lt;/camel.version\u0026gt; ... \u0026lt;/properties\u0026gt; Dependency Section \u0026lt;!-- Camel --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.camel.springboot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;camel-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Dependency Management Section \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- Camel BOM --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.camel.springboot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;camel-spring-boot-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${camel.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; Check CamelContext Let\u0026rsquo;s check if Camel is loading in out Spring Boot application for this let`s start the application\nRun the following command :\nmvn spring-boot:run Verify the console output you should see something like AbstractCamelContext - Apache Camel 3.2.0 (CamelContext: camel-1) is starting\n_ ____ _ | | __ __ ) ___ ___ | |_ | |/ / _ \\ / _ \\ / _ \\| __| | \u0026lt;| |_) | (_) | (_) | |_ |_|\\_\\____/ \\___/ \\___/ \\__| :: kboot-camel: :: Running Spring Boot: (v2.3.0.RC1) :: Active Profiles: default :: 2020-05-05 Tue 07:50:57.489 KbootCamelKt - Starting KbootCamelKt on MacBookPro with PID 20731 (/Users/morpheus/dev/kboot-camel/target/classes started by morpheus in /Users/morpheus/dev/kboot-camel) 2020-05-05 Tue 07:50:57.493 KbootCamelKt - No active profile set, falling back to default profiles: default 2020-05-05 Tue 07:50:58.261 trationDelegate$BeanPostProcessorChecker - Bean \u0026#39;org.apache.camel.spring.boot.CamelAutoConfiguration\u0026#39; of type [org.apache.camel.spring.boot.CamelAutoConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 2020-05-05 Tue 07:50:58.548 LRUCacheFactory - Detected and using LURCacheFactory: camel-caffeine-lrucache 2020-05-05 Tue 07:50:58.739 EndpointLinksResolver - Exposing 2 endpoint(s) beneath base path \u0026#39;/actuator\u0026#39; 2020-05-05 Tue 07:50:58.971 SpringBootRoutesCollector - Loading additional Camel XML routes from: classpath:camel/*.xml 2020-05-05 Tue 07:50:58.971 SpringBootRoutesCollector - Loading additional Camel XML rests from: classpath:camel-rest/*.xml 2020-05-05 Tue 07:50:58.985 DefaultManagementStrategy - JMX is enabled 2020-05-05 Tue 07:50:58.987 AbstractCamelContext - Apache Camel 3.2.0 (CamelContext: camel-1) is starting 2020-05-05 Tue 07:50:58.989 AbstractCamelContext - StreamCaching is not in use. If using streams then its recommended to enable stream caching. See more details at http://camel.apache.org/stream-caching.html 2020-05-05 Tue 07:50:58.989 AbstractCamelContext - Total 0 routes, of which 0 are started 2020-05-05 Tue 07:50:58.989 AbstractCamelContext - Apache Camel 3.2.0 (CamelContext: camel-1) started in 0.002 seconds 2020-05-05 Tue 07:50:59.139 NettyWebServer - Netty started on port(s): 8080 2020-05-05 Tue 07:50:59.142 KbootCamelKt - Started KbootCamelKt in 1.965 seconds (JVM running for 2.195) File Route File Builder Route @Component class FileRouteBuilder : RouteBuilder() { private val workDir =System.getenv(\u0026#34;PWD\u0026#34;) private val input = \u0026#34;$workDir/orders/in?include=order.xml\u0026#34; private val output = \u0026#34;$workDir/orders/out?fileExist=Fail\u0026#34; @Throws(Exception::class) override fun configure() { from(\u0026#34;file:$input\u0026#34;) .process(HeaderProcessor()) .to(\u0026#34;file:$output\u0026#34;) .log(\u0026#34;Camel body: \\${body.class} \\${body}\u0026#34;) } } To test the FileBuilderRoute you can copy the test XML files from my sample application on GitHub\n. ‚îú‚îÄ‚îÄ files ‚îÇ¬†‚îú‚îÄ‚îÄ order-2.xml ‚îÇ¬†‚îî‚îÄ‚îÄ order.xml in the folder orders/in after application start. The proceeded files will be then in the hidden folder .camel\norders ‚îú‚îÄ‚îÄ in ‚îÇ¬†‚îî‚îÄ‚îÄ .camel ‚îÇ¬†‚îú‚îÄ‚îÄ order-2.xml ‚îÇ¬†‚îî‚îÄ‚îÄ order.xml ‚îî‚îÄ‚îÄ out ‚îú‚îÄ‚îÄ 2019-01-28-order-2.xml ‚îî‚îÄ‚îÄ order.xml Processor Creat a HeaderProcessor that implement the function process from the interface org.apache.camel.Processor. there we parse the date with XPath.\nclass HeaderProcessor : Processor { private final val XPATH_DATE = \u0026#34;/order/orderDate/text()\u0026#34; override fun process(exchange: Exchange?) { val oderXml = exchange?.`in`?.body val orderDateTime = XPathBuilder.xpath(XPATH_DATE).evaluate(exchange?.context, oderXml) val formattedOrderDate = getFormattedData(orderDateTime = orderDateTime) exchange?.`in`?.setHeader(\u0026#34;orderDate\u0026#34;, formattedOrderDate) exchange?.`in`?.setHeader(\u0026#34;uuid\u0026#34;, UUID.randomUUID().toString().take(4)) } // TODO: 05.05.20 DirtyHarry Implementation private fun getFormattedData(orderDateTime: String) = DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd\u0026#34;) .format(LocalDateTime.ofInstant(Instant.parse(orderDateTime), ZoneOffset.UTC)) } Add HeaderProcessor class to our FileRouteBuilder with .process(HeaderProcessor()) We also change the filename in the to section with .to(\u0026quot;file:$workDir/orders/out?fileName=\\${header.orderDate}-\\${header.CamelFileName}\u0026quot;)\n@Component class FileRouteBuilder : RouteBuilder() { private val workDir =System.getenv(\u0026#34;PWD\u0026#34;) private val input = \u0026#34;$workDir/orders/in?include=order.xml\u0026#34; private val output = \u0026#34;$workDir/orders/out?fileExist=Fail\u0026#34; @Throws(Exception::class) override fun configure() { from(\u0026#34;file:$input\u0026#34;) .to(\u0026#34;file:$output\u0026#34;) .log(\u0026#34;Camel body: \\${body.class} \\${body}\u0026#34;) from(\u0026#34;file:$workDir/orders/in?include=order-.*xml\u0026#34;) .process(HeaderProcessor()) .to(\u0026#34;file:$workDir/orders/out?fileName=\\${header.orderDate}-\\${header.uuid}-\\${header.CamelFileName}\u0026#34;) .log(\u0026#34;Camel body: \\${body.class} \\${body}\u0026#34;) } } FTP Route FTP Route Builder Let\u0026rsquo;s update our pom file with the camel-ftp dependency.\n\u0026lt;!-- camel-ftp --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.camel\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;camel-ftp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; @Component class FtpRouteBuilder : RouteBuilder() { private val ftpEndpoint =\u0026#34;ftp.walkerit.ch\u0026#34; private val username =\u0026#34;public\u0026#34; private val password =\u0026#34;Public8852\u0026#34; private val workDir =System.getenv(\u0026#34;PWD\u0026#34;) private val output = \u0026#34;$workDir/orders/out?fileExist=Fail\u0026#34; @Throws(Exception::class) override fun configure() { from(\u0026#34;ftp://$ftpEndpoint?username=$username\u0026amp;password=$password\u0026amp;delete=true\u0026amp;include=order.*xml\u0026#34;) .log(\u0026#34;New File \\${header.CamleFileName} picked up from \\${header.CamleFileHost}\u0026#34;) .process(ExchangePrinter()) .to(\u0026#34;file://$output\u0026#34;) } } Processor class ExchangePrinter : Processor { private val log = LoggerFactory.getLogger(javaClass) @Throws(Exception::class) override fun process(exchange: Exchange?) { val body = exchange?.`in`?.body log.info(\u0026#34;Body: $body\u0026#34;) } } FTP Server Upload test xml file to public FTP server https://www.walkerit.ch/public-ftp with the script ftp-upload.sh.\nRun the following command :\n./ftp-upload.sh Start Application When you start the Application mvn spring-boot:run check the console output for New File picked up from:\n2020-05-06 Wed 07:32:39.101 KbootCamelKt - Started KbootCamelKt in 2.055 seconds (JVM running for 2.265) 2020-05-06 Wed 07:32:40.099 route3 - New File picked up from 2020-05-06 Wed 07:32:40.099 ExchangePrinter - Body: RemoteFile[order-ftp.xml] and the files will be downloaded in the out folder.\ntree . ‚îú‚îÄ‚îÄ in ‚îî‚îÄ‚îÄ out ‚îî‚îÄ‚îÄ order-ftp.xml Choice Route Choice Route Builder Lets creat a choice route aka switch.\nThis route will put the files in a folder of the publisher name. We will do this with xPATH\nprivate final val XPATH_SEARCH_ORLY = \u0026#34;order/orderItems/orderItem/orderItemPublisherName/text() = \u0026#39;ORly\u0026#39;\u0026#34; This will parse the xml file and deliver the files in the correct folder. For this you can put the sample files from the files folder in the in/publisher/ folder.\ntree . ‚îú‚îÄ‚îÄ in ‚îÇ¬†‚îî‚îÄ‚îÄ publisher ‚îÇ¬†‚îú‚îÄ‚îÄ pub-foo.xml ‚îÇ¬†‚îú‚îÄ‚îÄ pub-orly.xml ‚îÇ¬†‚îî‚îÄ‚îÄ pub-packt.xml ‚îî‚îÄ‚îÄ out ‚îî‚îÄ‚îÄ publisher ‚îú‚îÄ‚îÄ orly ‚îú‚îÄ‚îÄ others ‚îî‚îÄ‚îÄ packt @Component class ChoiceRouteBuilder : RouteBuilder() { private val workDir = System.getenv(\u0026#34;PWD\u0026#34;) private val input = \u0026#34;$workDir/orders/in/publisher?include=pub-.*xml\u0026#34; private val output = \u0026#34;$workDir/orders/out/publisher\u0026#34; private final val XPATH_SEARCH_ORLY = \u0026#34;order/orderItems/orderItem/orderItemPublisherName/text() = \u0026#39;ORly\u0026#39;\u0026#34; private final val XPATH_SEARCH_PACKT = \u0026#34;order/orderItems/orderItem/orderItemPublisherName/text() = \u0026#39;Packt\u0026#39;\u0026#34; @Throws(Exception::class) override fun configure() { from(\u0026#34;file:$input\u0026#34;) .to(\u0026#34;log:ordersReceived\u0026#34;) .choice() .`when`(xpath(XPATH_SEARCH_ORLY)) .log(\u0026#34;ORly received\u0026#34;) .to(\u0026#34;file:$output/orly\u0026#34;) .`when`(xpath(XPATH_SEARCH_PACKT)) .log(\u0026#34;Packt received\u0026#34;) .to(\u0026#34;file:$output/packt\u0026#34;) .otherwise() .log(\u0026#34;Other received\u0026#34;) .to(\u0026#34;file:$output/others\u0026#34;) .end() } } tree . ‚îú‚îÄ‚îÄ in ‚îÇ¬†‚îî‚îÄ‚îÄ publisher ‚îî‚îÄ‚îÄ out ‚îî‚îÄ‚îÄ publisher ‚îú‚îÄ‚îÄ orly ‚îÇ¬†‚îî‚îÄ‚îÄ pub-orly.xml ‚îú‚îÄ‚îÄ others ‚îÇ¬†‚îî‚îÄ‚îÄ pub-foo.xml ‚îî‚îÄ‚îÄ packt ‚îî‚îÄ‚îÄ pub-packt.xml References:\nGitHub Sample Project | Spring Tips Apache Camel | Camel Apache Spring Boot | Public FTP\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-05-04-camel-kotlin-spring-boot/","summary":"","title":"Apache Camel with Kotlin and Spring Boot"},{"content":"OSX Minikube Kubernetes If you use ohmyzsh is there a nice Plugin with some kubectl for more details check ohmyzsh plugins kubectl\nInstall Minikube with Ingress Controller brew install minikube Start Minikube minikube start üòÑ minikube v1.9.2 on Darwin 10.15.4 ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube ‚ú® Using the hyperkit driver based on existing profile üëç Starting control plane node m01 in cluster minikube üîÑ Restarting existing hyperkit VM for \u0026#34;minikube\u0026#34; ... üê≥ Preparing Kubernetes v1.18.0 on Docker 19.03.8 ... üåü Enabling addons: dashboard, default-storageclass, ingress, storage-provisioner üèÑ Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; Minkikube Status minikube status m01 host: Running kubelet: Running apiserver: Running kubeconfig: Configured Enable Ingress Addons We need the addon ingress this can be done with the followng command.\nminikube addons enable ingress Check minikube addons with minikube addons list if ingress is enabled.\nRun the following command:\nminikube addons list |-----------------------------|----------|--------------| | ADDON NAME | PROFILE | STATUS | |-----------------------------|----------|--------------| | dashboard | minikube | enabled ‚úÖ | | default-storageclass | minikube | enabled ‚úÖ | | efk | minikube | disabled | | freshpod | minikube | disabled | | gvisor | minikube | disabled | | helm-tiller | minikube | disabled | | ingress | minikube | enabled ‚úÖ | | ingress-dns | minikube | disabled | | istio | minikube | disabled | | istio-provisioner | minikube | disabled | | logviewer | minikube | disabled | | metrics-server | minikube | disabled | | nvidia-driver-installer | minikube | disabled | | nvidia-gpu-device-plugin | minikube | disabled | | registry | minikube | disabled | | registry-aliases | minikube | disabled | | registry-creds | minikube | disabled | | storage-provisioner | minikube | enabled ‚úÖ | | storage-provisioner-gluster | minikube | disabled | |-----------------------------|----------|--------------| Ingress Controller Let\u0026rsquo;s check if the Nginx Ingress Controller is up and running.\nRun the following command:\nk get pods -n kube-system | grep nginx-ingress nginx-ingress-controller-6d57c87cb9-rtgpk 1/1 Running 1 12m Create Namespace Let\u0026rsquo;s create first a dev name space where we deploy our sample application.\nRun the following command:\nk create ns dev namespace/dev created Switch Namespace Permanently With the following command we can switch the namespace permanently like oc project in a OpenShift environment. This command will save the namespace for all other kubectl commands in that context. This required the ohmyzsh plugins kubectl Plugin. Without this plugin will be the command kubectl config set-context --current --namespace=dev.\nkcn dev Create Deployment Let\u0026rsquo;s create a deployment now for a sample application with the name myapp with the image google-samples/hello-app:1.0 in the current namespace dev\nRun the following command:\nk create deployment myapp --image=gcr.io/google-samples/hello-app:1.0 deployment.apps/myapp created If you want see what deployments we have you can use kubectl get deployments command.\nRun the following command:\nk get deployments NAME READY UP-TO-DATE AVAILABLE AGE myapp 1/1 1 1 44m Let\u0026rsquo;s also check if the pod is running.\nRun the following command:\nk get pods -n dev -o wide --show-labels --field-selector=status.phase=Running NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELS myapp-75d86849b-2gbps 1/1 Running 0 67m 172.17.0.3 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; app=myapp,pod-template-hash=75d86849b Create Service To reach our application we have to create a service this can be easily done with the expose command and the NodePort\nk expose deployment myapp --port=8080 --type=NodePort Let\u0026rsquo;s check the service.\nRun the following command:\nk get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myapp NodePort 10.110.101.126 \u0026lt;none\u0026gt; 8080:30506/TCP 21m Test Service internal in a Cluster Let\u0026rsquo;s check the service with a small container based on the tutum/curl:alpine Docker image, which contains the curl command. Run the curl -s 'http://myapp:8080' command inside the container and redirect the output to the Terminal using the -i option. Delete the pod using the --rm option.\nRun the following command:\nk run -i --rm --restart=Never curl-client --image=tutum/curl:alpine --command -- curl -s \u0026#39;http://myapp:8080\u0026#39; Hello, world! Version: 1.0.0 Hostname: myapp-75d86849b-2gbps pod \u0026#34;curl-client\u0026#34; deleted This mean we can call a pod internal a cluster with the internal DNS.\nTest Service outside a Cluster Let\u0026rsquo;s call again the command k get svc to get the exposed port from out service. Run the following command again :\nk get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myapp NodePort 10.110.101.126 \u0026lt;none\u0026gt; 8080:30506/TCP 21m You see the exposed port is here 30506 now let\u0026rsquo;s get the IP address from out minikube with minikube ip\nminikube ip 192.168.64.18 Now let\u0026rsquo;s also call out service myapp with HTTPie or in a browser http://192.168.64.18:30506\nhttp http://192.168.64.18:30506 HTTP/1.1 200 OK Content-Length: 61 Content-Type: text/plain; charset=utf-8 Date: Sun, 03 May 2020 08:07:35 GMT Hello, world! Version: 1.0.0 Hostname: myapp-75d86849b-2gbps /etc/hosts To get on the service with the URL minikube.me we update now out local /etc/hosts file with IP address from your minikube minikube ip Run the following command:\necho $(minikube ip) minikube.me | sudo tee -a /etc/hosts Password: 192.168.64.11 minikube.me Verify it the IP is in the /etc/hosts with the following command:\ncat /etc/hosts | tail -n 1 192.168.64.11 minikube.me Now you can also call the service wit the DNS http://minikube.me:30506\nhttp http://minikube.me:30506 HTTP/1.1 200 OK Content-Length: 61 Content-Type: text/plain; charset=utf-8 Date: Sun, 03 May 2020 08:12:51 GMT Hello, world! Version: 1.0.0 Hostname: myapp-75d86849b-2gbps Nginx Ingress Now is time to create our nginx ingress for this we creat a file named nginx-ingess.yaml where we point / to the myapp service on internal port 8080.\nRun the following command:\necho \u0026#34;apiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress namespace: dev annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - host: minikube.me http: paths: - path: / backend: serviceName: myapp servicePort: 8080\u0026#34; | \u0026gt; nginx-ingess.yaml Now let\u0026rsquo;s also take a new alias in the game kaf is the alias for kubectl -f and let\u0026rsquo;s apply the nginx-ingess.yaml file. For this run the following command:\nkaf nginx-ingess.yaml ingress.networking.k8s.io/nginx-ingress created With kubectl get ingress you can list all ingress in the namespace.\nk get ingress NAME CLASS HOSTS ADDRESS PORTS AGE nginx-ingress \u0026lt;none\u0026gt; minikube.me 80 22s Test Ingress Open a browser now http://minikube.me or with HTTPie in a terminal.\nhttp http://minikube.me HTTP/1.1 200 OK Connection: keep-alive Content-Length: 61 Content-Type: text/plain; charset=utf-8 Date: Sun, 03 May 2020 09:18:45 GMT Server: openresty/1.15.8.2 Hello, world! Version: 1.0.0 Hostname: myapp-75d86849b-v72vs Now let\u0026rsquo;s also deploy a second application and expose it as service. Configure the nginx ingress also to the second service.\nk create deployment myapp2 --image=gcr.io/google-samples/hello-app:2.0 k expose deployment myapp2 --port=8080 --type=NodePort Update the nginx-ingess.yaml file with vi\n- path: /v2/* backend: serviceName: myapp2 servicePort: 8080 Check the nginx-ingess.yaml file with cat nginx-ingess.yaml\napiVersion: networking.k8s.io/v1beta1 # for versions before 1.14 use extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress namespace: dev annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: minikube.me http: paths: - path: / backend: serviceName: myapp servicePort: 8080 - path: /v2 backend: serviceName: myapp2 servicePort: 8080 Apply the changes with the following command:\nkaf nginx-ingess.yaml ingress.networking.k8s.io/nginx-ingress configured Now we have one Ingress who point to 2 individual services. Verify it with HTTPie or with the browser http://minikube.me/ will give back Version: 1.0.0\nhttp http://minikube.me/ HTTP/1.1 200 OK Connection: keep-alive Content-Length: 61 Content-Type: text/plain; charset=utf-8 Date: Sun, 03 May 2020 09:31:12 GMT Server: openresty/1.15.8.2 Hello, world! Version: 1.0.0 Hostname: myapp-75d86849b-v72vs On the second route http://minikube.me/v2 we get back Version: 2.0.0\nhttp http://minikube.me/v2 HTTP/1.1 200 OK Connection: keep-alive Content-Length: 63 Content-Type: text/plain; charset=utf-8 Date: Sun, 03 May 2020 09:32:23 GMT Server: openresty/1.15.8.2 Hello, world! Version: 2.0.0 Hostname: myapp2-548b98644f-rqqmp kubectl Cheat Sheet k get pods -o wide --show-labels --field-selector=status.phase=Running -w | grep -vi \u0026#34;Terminating\u0026#34; NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELS web-6785d44d5-vb2wb 1/1 Running 1 19h 172.17.0.4 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; app=web,pod-template-hash=6785d44d5 web2 1/1 Running 1 19h 172.17.0.3 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; run=web2 web2-8474c56fd-wk4kh 1/1 Running 1 18h 172.17.0.5 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; app=web2,pod-template-hash=8474c56fd Namespaces kubectl get ns k get ns kgns k get ns NAME STATUS AGE default Active 19h kube-node-lease Active 19h kube-public Active 19h kube-system Active 19h Delete with given label k delete all -l app=web Troubleshooting sudo dscacheutil -flushcache;sudo killall -HUP mDNSResponder References:\ningress-minikube\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-05-01-minikube-ingress-controller/","summary":"","title":"Kubernetes Ingress with Ngnix Ingress Controller"},{"content":"Create Project Let\u0026rsquo;s create a Sample Application with Kotlin and Reactive Spring Boot with the spring inializr Rest Endpoint. We will take the latest and greates Spring Boot version 2.3.0.M4 and language kotlin with the following dependencies:\nactuator webflux cloud-resilience4j http https://start.spring.io/starter.tgz \\ dependencies==actuator,webflux,cloud-resilience4j \\ description==\u0026#34;Demo project Kotlin Spring Boot with Resilience4j\u0026#34; \\ applicationName==Resilience4jApplication \\ name==kboot-resilience4j \\ groupId==ch.keepcalm \\ artifactId==kboot-resilience4j \\ packageName==ch.keepcalm.demo \\ javaVersion==11 \\ language==kotlin \\ baseDir==kboot-resilience4j| tar -xzvf - Add Customer Banner\nhttp https://raw.githubusercontent.com/marzelwidmer/marzelwidmer.github.io/master/img/banner.txt \\ \u0026gt; kboot-resilience4j/src/main/resources/banner.txt Configure spring.applicatin.name\necho \u0026#34;spring: application: name: kboot-resilience4j\u0026#34; | \u0026gt; kboot-resilience4j/src/main/resources/application.yaml Remove application.properties\nrm kboot-resilience4j/src/main/resources/application.properties See also my other post Spring Initializr and HTTPie\nDomain Model Let\u0026rsquo;s start with the Movie domain class with the following properties.\nname year description data class Movie(val id: String? = UUID.randomUUID().toString(), val name: String, val year: Year, val description: String) Service Now we create the service class MovieService who hold some hard coded movies in a list. The amazing functions:\nGet all Movies Get a random list of Movies Get a Movie by his name Get a Movie by his ID @Service class MovieService { private val movies = listOf( Movie(name = \u0026#34;Matrix\u0026#34;, year = Year.of(1999), description = \u0026#34;A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers.\u0026#34;), Movie(name = \u0026#34;The Godfather\u0026#34;, year = Year.of(1972), description = \u0026#34;The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.\u0026#34;), Movie(name = \u0026#34;Casablanca\u0026#34;, year = Year.of(1942), description = \u0026#34;A cynical American expatriate struggles to decide whether or not he should help his former lover and her fugitive husband escape French Morocco.\u0026#34;), Movie(name = \u0026#34;Rocky\u0026#34;, year = Year.of(1976), description = \u0026#34;A small-time boxer gets a supremely rare chance to fight a heavy-weight champion in a bout in which he strives to go the distance for his self-respect.\u0026#34;) ) fun randomMovie() = Mono.just(movies[kotlin.random.Random.nextInt(movies.size)]) fun movies() = Flux.just(movies) fun movieById(id: String) = Mono.just(movies.first { it.id == id }) fun movieByName(name: String?): Mono\u0026lt;Movie\u0026gt; { name?.map { movies.firstOrNull() { it.name.toLowerCase() == name.toLowerCase() }?.let { return Mono.just(it) } }.isNullOrEmpty().apply { return Mono.error(IllegalArgumentException(\u0026#34;Movie was not found.\u0026#34;)) } } } Rest API I think now is time to create a REST API /movies/random with the Reactive router Kotlin DSL. A easy way to create a WebFlux.fn RouterFunction Because we have more then one endpoint under /movies we use the \u0026quot;/movies\u0026quot;.nest router function.\nWe use also our service MovieService so we need a Bean Reference to it.\nval service = ref\u0026lt;MovieService\u0026gt;()\nüí°: Take care of the order in the Route definiton \u0026ldquo;/\u0026rdquo; have to be at latest position.\nfun main(args: Array\u0026lt;String\u0026gt;) { runApplication\u0026lt;Resilience4jApplication\u0026gt;(*args) { addInitializers( beans { bean { router { \u0026#34;movies\u0026#34;.nest { val service = ref\u0026lt;MovieService\u0026gt;() //http :8080/movies/random GET(\u0026#34;/random\u0026#34;) { ok().body(service.randomMovie()) } //http :8080/movies/ name==\u0026#34;Rocky\u0026#34; -vv queryParam(\u0026#34;name\u0026#34;, { true }) { ok().body(service.movieByName(name = it.queryParam(\u0026#34;name\u0026#34;).get())) } //http :8080/movies/c7f399bc-ff4c-4a2f-bddf-d92d53a96df2 GET(\u0026#34;/{id}\u0026#34;) { ok().body(service.movieById(id = it.pathVariable(\u0026#34;id\u0026#34;))) } //http :8080/movies/random GET(\u0026#34;/\u0026#34;) { ok().body(service.movies()) } } } } } ) } } Test Rest API No is time to make some calls from the terminal with the HTTPie or in a Browser. First start the application e.g. with mvn spring-boot:run.\nThen lets call our amazing endpoints with the HTTPie or Browser.\nGet all Movies http://localhost:8080/movies\nhttp :8080/movies/ HTTP/1.1 200 OK Content-Type: application/json transfer-encoding: chunked [ [ { \u0026#34;description\u0026#34;: \u0026#34;A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;c7f399bc-ff4c-4a2f-bddf-d92d53a96df2\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Matrix\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1999\u0026#34; }, { \u0026#34;description\u0026#34;: \u0026#34;The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;0c7a74fc-b735-41a3-a383-72b9dad7d608\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;The Godfather\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1972\u0026#34; }, { \u0026#34;description\u0026#34;: \u0026#34;A cynical American expatriate struggles to decide whether or not he should help his former lover and her fugitive husband escape French Morocco.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;15a9e15d-4a5e-4b8a-a7c5-d34b0d5d0879\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Casablanca\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1942\u0026#34; }, { \u0026#34;description\u0026#34;: \u0026#34;A small-time boxer gets a supremely rare chance to fight a heavy-weight champion in a bout in which he strives to go the distance for his self-respect.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;f6fb4a62-6d84-434e-8e2c-70e4c1e7ab2c\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Rocky\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1976\u0026#34; } ] ] Get a random list of Movies http://localhost:8080/movies/random\nhttp :8080/movies/random HTTP/1.1 200 OK Content-Length: 240 Content-Type: application/json { \u0026#34;description\u0026#34;: \u0026#34;A cynical American expatriate struggles to decide whether or not he should help his former lover and her fugitive husband escape French Morocco.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;5dd310b8-8d51-4a1e-a20c-790fec00029f\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Casablanca\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1942\u0026#34; } Get a Movie by his name http://localhost:8080/movies/?name=Rocky\nhttp :8080/movies name==\u0026#34;Rocky\u0026#34; -v GET /movies?name=Rocky HTTP/1.1 Accept: */* Accept-Encoding: gzip, deflate Connection: keep-alive Host: localhost:8080 User-Agent: HTTPie/2.0.0 HTTP/1.1 200 OK Content-Length: 242 Content-Type: application/json { \u0026#34;description\u0026#34;: \u0026#34;A small-time boxer gets a supremely rare chance to fight a heavy-weight champion in a bout in which he strives to go the distance for his self-respect.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;5cae75c6-d18a-495b-a80f-60bdd0763eb1\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Rocky\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1976\u0026#34; } Get a Movie by his ID http://localhost:8080/movies/5dd310b8-8d51-4a1e-a20c-790fec00029f\nhttp :8080/movies/5dd310b8-8d51-4a1e-a20c-790fec00029f HTTP/1.1 200 OK Content-Length: 240 Content-Type: application/json { \u0026#34;description\u0026#34;: \u0026#34;A cynical American expatriate struggles to decide whether or not he should help his former lover and her fugitive husband escape French Morocco.\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;5dd310b8-8d51-4a1e-a20c-790fec00029f\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Casablanca\u0026#34;, \u0026#34;year\u0026#34;: \u0026#34;1942\u0026#34; } Search for a not existing Movie Now let\u0026rsquo;s also search for http://localhost:8080/movies/?name=Creed is also a great movie but this one is not yet in our MovieService included we will get the following exception.\nhttp :8080/movies name==\u0026#34;Creed\u0026#34; -v GET /movies?name=Creed HTTP/1.1 Accept: */* Accept-Encoding: gzip, deflate Connection: keep-alive Host: localhost:8080 User-Agent: HTTPie/2.0.0 HTTP/1.1 500 Internal Server Error Content-Length: 165 Content-Type: application/json { \u0026#34;error\u0026#34;: \u0026#34;Internal Server Error\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Movie was not found.\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/movies\u0026#34;, \u0026#34;requestId\u0026#34;: \u0026#34;61569a89-3\u0026#34;, \u0026#34;status\u0026#34;: 500, \u0026#34;timestamp\u0026#34;: \u0026#34;2020-04-19T21:13:10.403+00:00\u0026#34; } Configure CircuitBreaker üòé Cool stuff üòé let\u0026rsquo;s implement the CircuitBreaker with Resilinece4j. Let\u0026rsquo;s configure the ReactiveCircuitBreaker Bean from ReactiveResilience4JCircuitBreakerFactory with a name movie-service.\nbean { ReactiveResilience4JCircuitBreakerFactory() .create(\u0026#34;movie-service\u0026#34;) } to be continued .. The example source code can be found here GitHub\nüí° Logger Configuration: logging.pattern.console: \u0026ldquo;%clr(%d{yyyy-MM-dd E HH:mm:ss.SSS}){blue} %clr(%-40.40logger{0}){magenta} - %clr(%m){green}%n\u0026rdquo;\nReferences:\nResilience4j docs\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-04-18-resilience4j-circuitbreaker/","summary":"","title":"Reactive Spring Boot with Resilience4j CircuitBreaker"},{"content":"This sample show how easy you can put on an existing API the Spring Cloud Gateway as kind of SideCar where you can manage your Security, Logging etc. Or just provide an other Endpoint URL like in this sample.\nLet\u0026rsquo;s create a Service with a Reactive Spring Boot Application and MongoDB and a Rest Endpoint.\nWe start with the MongoDB document class Customer and a ReactiveCrudRepository interface CustomerRepository\n@Document data class Customer(@Id val id: String = UUID.randomUUID().toString(), val name: String) interface CustomerRepository : ReactiveCrudRepository\u0026lt;Customer, String\u0026gt; Now let\u0026rsquo;s also create a service class CustomerService for it where we provide the following functionality.\nsave findAll deleteAll findById @Service class CustomerService(private val customerRepository: CustomerRepository) { fun save(customer: Customer) = customerRepository.save(customer) fun findAll() = customerRepository.findAll() fun deleteAll() = customerRepository.deleteAll() fun findById(id: String) = customerRepository.findById(id) } That we have some data we create a little functionality on application start with the ApplicationRunner from Spring Boot.\nLet\u0026rsquo;s create a Bean definition for the ApplicationRunner that delete first all entries and then save some sample values to it.\nfun main(args: Array\u0026lt;String\u0026gt;) { runApplication\u0026lt;SidecarGatewayApplication\u0026gt;(*args) { addInitializers( beans { bean { ApplicationRunner { val customerService = ref\u0026lt;CustomerService\u0026gt;() customerService // first cleanUp Database .deleteAll() // create a list of Customers .thenMany( listOf(\u0026#34;John\u0026#34;, \u0026#34;Jane\u0026#34;, \u0026#34;Jack\u0026#34;) .toFlux() .map { Customer(name = it) }) // Save it to the Database .flatMap { customerService.save(it) } // Search all entries .thenMany(customerService.findAll()) // subscribe - let`s do the work... .subscribe { log.info(\u0026#34;--\u0026gt; $it\u0026#34;) } } } } ) } } Now we have some data in our MongoDB I think now is time to create a other Bean with the Kotlin DSL that provide a Rest endpoint. For this we create an Router that will provide the following endpoints.\n/customers /customers/{id} // Rest API bean { router { val customerService = ref\u0026lt;CustomerService\u0026gt;() GET(\u0026#34;/customers\u0026#34;) { ok().body(customerService.findAll()) } GET(\u0026#34;/customers/{id}\u0026#34;) { ok().body(customerService.findById(it.pathVariable(\u0026#34;id\u0026#34;))) } } } When we start now our application we can call the endpoint and hopefully we get a result like below.\nmvn spring-boot:run http :8080/customers HTTP/1.1 200 OK Content-Type: application/json transfer-encoding: chunked [ { \u0026#34;id\u0026#34;: \u0026#34;a16c9582-0f40-4a7b-a566-372a56c3d5c8\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;944c3752-55c5-4ede-bc09-e02a5e47b390\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;478ce3f9-0eff-4018-a056-0656cd2c5ad4\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jack\u0026#34; } ] Now let\u0026rsquo;s create a sidecar with Spring cloud Gateway that provide another Rest API /api/customers and /api/customers/{id} Let\u0026rsquo;s create an additional response header X-AnotherHeader with the value SideCar as well.\n// Gateway Sidecar API // http -v :8080/api/customers // Gateway - Sidecar bean { ref\u0026lt;RouteLocatorBuilder\u0026gt;() .routes { // http -v :8080/api/customers route(\u0026#34;sidecar-api\u0026#34;) { path(\u0026#34;/api/**\u0026#34;) filters { rewritePath(\u0026#34;api(?\u0026lt;segment\u0026gt;/?.*)\u0026#34;, \u0026#34;/$\\\\{segment}\u0026#34;) } uri(\u0026#34;http://localhost:8080\u0026#34;) } } } When we call now the EndPoint /api/customers we expect that we get the result from before and the additional ResponsHeader with X-AnotherHeader: SideCar\nhttp -v :8080/api/customers GET /api/customers HTTP/1.1 Accept: */* Accept-Encoding: gzip, deflate Connection: keep-alive Host: localhost:8080 User-Agent: HTTPie/2.0.0 HTTP/1.1 200 OK Content-Type: application/json X-AnotherHeader: SideCar transfer-encoding: chunked [ { \u0026#34;id\u0026#34;: \u0026#34;a36f75ae-a97f-41ba-9b38-59a5a6d38055\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;c0b25559-13ad-4b6b-ae25-adbcd898db82\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jane\u0026#34; }, { \u0026#34;id\u0026#34;: \u0026#34;2a3a1350-4bea-4504-b732-5b7c97602ebb\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Jack\u0026#34; } ] With the Kotlin DSL Route definition is it also easy to create routes only for specifics Spring profiles.\nThe following command will start the application with the default profile.\nmvn spring-boot:run Spring Cloud Gateway provide with the Actuator library an endpoint to check the configured routes. Also let\u0026rsquo;s check first our Routing Table whe we start the application with the default Spring Profile.\nhttp :8080/actuator/gateway/routes HTTP/1.1 200 OK Content-Type: application/json transfer-encoding: chunked [ { \u0026#34;filters\u0026#34;: [ \u0026#34;[[RewritePath api(?\u0026lt;segment\u0026gt;/?.*) = \u0026#39;/${segment}\u0026#39;], order = 0]\u0026#34; ], \u0026#34;order\u0026#34;: 0, \u0026#34;predicate\u0026#34;: \u0026#34;Paths: [/api/**], match trailing slash: true\u0026#34;, \u0026#34;route_id\u0026#34;: \u0026#34;sidecar-api\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;http://localhost:8080\u0026#34; } ] Now create a Route just for a specific Spring Profile foo\nrunApplication\u0026lt;SidecarGatewayApplication\u0026gt;(*args) { addInitializers( beans { // Profile profile(\u0026#34;foo\u0026#34;) { // Gateway - Sidecar bean { ref\u0026lt;RouteLocatorBuilder\u0026gt;() .routes { // http -v :8080/ route(\u0026#34;sidecar-root-to-customers-api\u0026#34;) { path(\u0026#34;/**\u0026#34;) filters { rewritePath(\u0026#34;/(?\u0026lt;segment\u0026gt;/?.*)\u0026#34;, \u0026#34;/customers/$\\\\{segment}\u0026#34;) } uri(\u0026#34;http://localhost:8080/\u0026#34;) } } } } } ) } Start the application with the foo profile with -Dspring-boot.run.profiles=foo and check again the Routing Table.\nmvn spring-boot:run -Dspring-boot.run.profiles=foo http :8080/actuator/gateway/routes HTTP/1.1 200 OK Content-Type: application/json transfer-encoding: chunked [ { \u0026#34;filters\u0026#34;: [ \u0026#34;[[RewritePath api(?\u0026lt;segment\u0026gt;/?.*) = \u0026#39;/${segment}\u0026#39;], order = 0]\u0026#34; ], \u0026#34;order\u0026#34;: 0, \u0026#34;predicate\u0026#34;: \u0026#34;Paths: [/api/**], match trailing slash: true\u0026#34;, \u0026#34;route_id\u0026#34;: \u0026#34;sidecar-api\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;http://localhost:8080\u0026#34; }, { \u0026#34;filters\u0026#34;: [ \u0026#34;[[RewritePath /(?\u0026lt;segment\u0026gt;/?.*) = \u0026#39;/customers/${segment}\u0026#39;], order = 0]\u0026#34; ], \u0026#34;order\u0026#34;: 0, \u0026#34;predicate\u0026#34;: \u0026#34;Paths: [/**], match trailing slash: true\u0026#34;, \u0026#34;route_id\u0026#34;: \u0026#34;sidecar-root-to-customers-api\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;http://localhost:8080/\u0026#34; } ] The example source code can be found here GitHub kotlin-sidecar-gateway\n","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-04-12-cloud-gateway-sidecar/","summary":"","title":"Spring Cloud Gateway"},{"content":"Create Kotlin Maven Project with HTTPie from start.spring.io Let\u0026rsquo;s create and extract a Maven Kotlin project with some dependencies actuator data-mongodb-reactive webflux and cloud-gateway The https://start.spring.io\nhttp https://start.spring.io/starter.tgz \\ dependencies==actuator,data-mongodb-reactive,webflux,cloud-gateway \\ description==\u0026#34;Demo project Kotlin Sidecar Gateway\u0026#34; \\ applicationName==SidecarGatewayApplication \\ name==kotlin-sidecar-gateway \\ groupId==ch.keepcalm \\ artifactId==kotlin-sidecar-gateway \\ packageName==ch.keepcalm.demo \\ javaVersion==11 \\ language==kotlin \\ baseDir==kotlin-sidecar-gateway | tar -xzvf - Banner Download Banner in the src/main/resources folder.\nhttp https://raw.githubusercontent.com/marzelwidmer/marzelwidmer.github.io/master/img/banner.txt \\ \u0026gt; kotlin-sidecar-gateway/src/main/resources/banner.txt Spring Application Name Configure spring.applicatin.name\necho \u0026#34;spring: application: name: kotlin-sidecar-gateway\u0026#34; | \u0026gt; kotlin-sidecar-gateway/src/main/resources/application.yaml Remove application.properties\nrm kotlin-sidecar-gateway/src/main/resources/application.properties ","permalink":"https://blog.marcelwidmer.org/blog/2020/2020-04-12-spring-initializr/","summary":"","title":"Create Kotlin Project with Spring Initializr and HTTPie"},{"content":" Commands:\nA command tells our application to do something Events:\nAn event is a notification of something that has happened. Query:\nQueries could be simplified by storing a copy of the data in a form easily In many cases, updating the query models can happen asynchronously from processing the transaction: eventual consistency Projection :\nOptimized for the specific read use-cases (e.g. screens, API methods) Many separated ones instead of one big one Use carious technologies as appropriate (RDMS, Elastic, Mongo etc.) Setup Infrastructure Axon Server docker run -it --rm --name axonserver -p 8024:8024 -p 8124:8124 axoniq/axonserver Postgresql docker run -it --rm --name postgres -p 5432:5432 -e POSTGRES_USER=account -e POSTGRES_PASSWORD=secret postgres:12 Connect to Postgresql docker exec -it postgres psql -Uaccount -a account Select * from table SELECT * FROM account; id | initial_value | remaining_value --------------------------------------+---------------+----------------- 8b76ad5b-e2f8-4411-bc89-2fda39ed3abd | 100 | 100 b8c5e738-f997-42bf-928b-e32e8c8b182d | 100 | 100 85ba51f2-a7cf-4b8c-9a7c-2c20543305df | 100 | 100 376fd5dd-cafe-45fd-9af9-b52f1315d7f6 | 100 | 100 a7fc627f-9d35-4752-82b8-1f7ff1135e40 | 100 | 100 1914e15c-ebf1-4977-b2f5-7e9bf26a96f6 | 100 | 30 9377f5c4-be39-493f-9b2e-aebb712015b1 | 100 | 30 2e655109-3049-4d3c-bd27-0b1c00310513 | 100 | 30 555fb817-3229-4a86-8b3c-7fe092390a64 | 100 | 0 adf5be06-fe4b-4bbb-bc70-c1ef1b59d146 | 100 | 30 e2a91918-6c20-4772-a6ae-ca5a1cbe809c | 100 | 0 1e7d13b1-6c25-4877-9f2a-3bf10e227abe | 100 | 0 OpenShift Create Project $ oc new-project axon --display-name=\u0026#34;AxonIQ Application\u0026#34; Create OpenShift Docker Image On OpenShift are some restriction for this we have to patch the axoniq/axonserver Docker image.\nBuild Axon Docker Image Patch Docker Image from Axon to run on Openshift.\n$ docker build --no-cache -t c3smonkey/axonserver-openshift:latest openshift/dockerfiles/ Docker Login $ docker login Docker Push $ docker push c3smonkey/axonserver-openshift:latest Deploy Axon Server $ oc apply -f openshift/axonserver.yaml Configure Axon Server EndPoint If you deploy AxonServer on the same Project namespace you can configure the Spring Boot application in the application.yml file like below\naxon: axonserver: servers: axonserver:8124 Deploy Application Deploy bank-account-command and bank-account-query Spring Boot Application. with the Fabric8 Maven Plug-In in the axon project (namespace)\n$ ./mvnw clean fabric8:deploy -pl bank-account-command,bank-account-query -Dfabric8.namespace=axon Scale ReplicaSet of Axon $ oc scale statefulsets axonserver --replicas=2 Check the physical files Search for axon-data PersistentVolumeClaim PVC.\n$ oc get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE axon-data-axonserver-0 Bound vol83 500Gi RWO,RWX 1h Describe the PersistentVolume PV for the Axon\n$ oc describe pv/vol83 Name: vol83 Labels: \u0026lt;none\u0026gt; Annotations: pv.kubernetes.io/bound-by-controller: yes Finalizers: [kubernetes.io/pv-protection] StorageClass: Status: Bound Claim: axon/axon-data-axonserver-0 Reclaim Policy: Retain Access Modes: RWO,RWX Capacity: 500Gi Node Affinity: \u0026lt;none\u0026gt; Message: Source: Type: HostPath (bare host directory volume) Path: /mnt/data/vol83 HostPathType: Events: \u0026lt;none\u0026gt; Then login to your Cluster there you will find the data.\n$ ls -la /mnt/data/vol83 total 80 drwxrwxrwx. 3 root root 4096 Oct 22 11:04 . drwxr-xr-x. 202 root root 4096 Sep 9 17:39 .. -rw-r--r--. 1 1000080000 root 69632 Oct 22 11:04 axonserver-controldb.mv.db drwxr-xr-x. 2 1000080000 root 4096 Oct 22 11:04 default ","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-10-22-axon/","summary":"","title":"AxonIQ"},{"content":"Create Project We are going to use the CLI to create some projects. Let\u0026rsquo;s create our projects first:\n$ oc login $ oc new-project development --display-name=\u0026#34;Development Environment\u0026#34; $ oc new-project testing --display-name=\u0026#34;Testing Environment\u0026#34; $ oc new-project production --display-name=\u0026#34;Production Environment\u0026#34; $ oc new-project jenkins --display-name=\u0026#34;Jenkins CI/CD\u0026#34; Install Jenkins Create a Jenkins in the Jenkins CI/CD project with some storage. Take the Jenkins from the catalog and set some more memory and volume capacity on it. Everything else we let the default values. Login with your Openshift account to the Jenkins BlueOcean.\nConfigure Jenkins Maven Slave - Concurrency Limit Let\u0026rsquo;s configure out Maven-Slave concurrency limit to 5 in order that we later want build more then one project. Please go to the Jenkins Configuration Page https://\u0026lt;jenkins\u0026gt;/configure in the section Cloud/Kubernetes Pod Template and search for the Maven Pod.\nInstall Jenkins with CLI $ oc new-app jenkins-persistent --name jenkins --param ENABLE_OAUTH=true \\ --param MEMORY_LIMIT=2Gi --param VOLUME_CAPACITY=4Gi -n jenkins Jenkins File From Source Repository The pipeline Jenkinsfile is provided in the source repository.\nAdd Edit Role To ServiceAccount Jenkins Let‚Äôs add in RBAC to our projects to allow the different service accounts to build, pro‚Äê mote, and tag images. First we will allow the cicd project‚Äôs Jenkins service account edit access to all of our projects:\n$ oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n development $ oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n testing $ oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n production Add Role To Group That we can pull our image from testing and production environment from the development registry. This is needed for pulling the Images across the projects.\n$ oc policy add-role-to-group system:image-puller system:serviceaccounts:testing \\ -n development $ oc policy add-role-to-group system:image-puller system:serviceaccounts:production \\ -n development Deploy Application Let\u0026rsquo;s deploy first the application with the S2i strategy. before we will create the delivery pipeline.\nDevelopment Environment Deployment Let\u0026rsquo;s change first the project to to development with the oc project development command.\n$ oc project development Now using project \u0026#34;development\u0026#34; on server \u0026#34;https://console.c3smonkey.ch:8443\u0026#34;. Creat a new app with `oc new-app` We will use here the fabric8/s2i-java to deploy our application and will point it to the master branch with the command oc new-app We also want expose the service oc expose svc/catalog-service to get a URL with the command oc get route catalog-service we will see the URL on the terminal.\n$ oc new-app fabric8/s2i-java:latest-java11~https://github.com/marzelwidmer/catalog-service.git#master; \\ oc expose svc/catalog-service; \\ oc get route catalog-service --\u0026gt; Found Docker image 6414174 (7 weeks old) from Docker Hub for \u0026#34;fabric8/s2i-java:latest-java11\u0026#34; Java Applications ----------------- Platform for building and running plain Java applications (fat-jar and flat classpath) Tags: builder, java * An image stream tag will be created as \u0026#34;s2i-java:latest-java11\u0026#34; that will track the source image * A source build using source code from https://github.com/marzelwidmer/catalog-service.git#master will be created * The resulting image will be pushed to image stream tag \u0026#34;catalog-service:latest\u0026#34; * Every time \u0026#34;s2i-java:latest-java11\u0026#34; changes a new build will be triggered * This image will be deployed in deployment config \u0026#34;catalog-service\u0026#34; * Ports 8080/tcp, 8778/tcp, 9779/tcp will be load balanced by service \u0026#34;catalog-service\u0026#34; * Other containers can access this service through the hostname \u0026#34;catalog-service\u0026#34; --\u0026gt; Creating resources ... imagestream.image.openshift.io \u0026#34;s2i-java\u0026#34; created imagestream.image.openshift.io \u0026#34;catalog-service\u0026#34; created buildconfig.build.openshift.io \u0026#34;catalog-service\u0026#34; created deploymentconfig.apps.openshift.io \u0026#34;catalog-service\u0026#34; created service \u0026#34;catalog-service\u0026#34; created --\u0026gt; Success Build scheduled, use \u0026#39;oc logs -f bc/catalog-service\u0026#39; to track its progress. Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: \u0026#39;oc expose svc/catalog-service\u0026#39; Run \u0026#39;oc status\u0026#39; to view your app. route.route.openshift.io/catalog-service exposed NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD catalog-service catalog-service-development.apps.c3smonkey.ch catalog-service 8080-tcp None Now take a look in the OpenShift Console project development\nLet\u0026rsquo;s take a look what the S2i crated for us. This can be done with the following command oc get all -n development --selector app=catalog-service.\n$ oc get all -n development --selector app=catalog-service NAME READY STATUS RESTARTS AGE pod/catalog-service-1-2rv5r 1/1 Running 0 56m NAME DESIRED CURRENT READY AGE replicationcontroller/catalog-service-1 1 1 1 56m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/catalog-service ClusterIP 172.30.216.240 \u0026lt;none\u0026gt; 8080/TCP,8778/TCP,9779/TCP 57m NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/catalog-service 1 1 1 config,image(catalog-service:latest) NAME TYPE FROM LATEST buildconfig.build.openshift.io/catalog-service Source Git@master 1 NAME TYPE FROM STATUS STARTED DURATION build.build.openshift.io/catalog-service-1 Source Git@b49dff4 Complete About an hour ago 1m10s NAME DOCKER REPO TAGS imagestream.image.openshift.io/catalog-service docker-registry.default.svc:5000/development/catalog-service latest imagestream.image.openshift.io/s2i-java docker-registry.default.svc:5000/development/s2i-java latest-java11 NAME HOST/PORT PATH SERVICES PORT WILDCARD route.route.openshift.io/catalog-service catalog-service-development.apps.c3smonkey.ch catalog-service 8080-tcp None Test API on Development Now let\u0026rsquo;s test the amazing /api/v1/animals/rando API from catalog-service by hitting the following Rest endpoint 50 times. in a bash shell with the following command. for x in (seq 50); http \u0026quot;http://catalog-service-development.apps.c3smonkey.ch/api/v1/animals/random\u0026quot;; end\n$ ~ üê† for x in (seq 50); \\ http \u0026#34;http://catalog-service-development.apps.c3smonkey.ch/api/v1/animals/random\u0026#34;; \\ end HTTP/1.1 200 OK Cache-control: private Content-Length: 14 Content-Type: text/plain;charset=UTF-8 Set-Cookie: 1e5e1500c4996e7978ef9efb67d863a1=1e12d12873c24c5c17782f3da537ed6a; path=/; HttpOnly Burrowing Frog HTTP/1.1 200 OK Cache-control: private Content-Length: 14 Content-Type: text/plain;charset=UTF-8 Set-Cookie: 1e5e1500c4996e7978ef9efb67d863a1=1e12d12873c24c5c17782f3da537ed6a; path=/; HttpOnly Dogo Argentino HTTP/1.1 200 OK Testing Environment Deployment Let\u0026rsquo;s change first to the testing project with oc project testing. We remember that we have in our setup only one docker registry from this registry we want promote our Docker images to other projects in our OpenShift Cluster setup. The access is now available because we did the Add Role To Group. Now let\u0026rsquo;s take a look at the ImageStream in the project development with the following command oc get is -n development we will get the docker registry we need to create the a deployment configuration in the project testing. We are searching for the catalog-service docker registry.\n$ oc get is -n development NAME DOCKER REPO TAGS UPDATED catalog-service docker-registry.default.svc:5000/development/catalog-service latest About an hour ago s2i-java docker-registry.default.svc:5000/development/s2i-java latest-java11 About an hour ago Now let\u0026rsquo;s create a deployment configuration for the promoteQA tag with oc create dc catalog-service --image=docker-registry.default.svc:5000/development/catalog-service:promoteQA Our Jenkins pipeline is configured to interact with this tag to promote between the environments.\n$ oc create dc catalog-service --image=docker-registry.default.svc:5000/development/catalog-service:promoteQA deploymentconfig.apps.openshift.io/catalog-service created Now we have to expose the dc/catalog-service with the port 8080 who our Spring Boot App is running on.\nThis easy done with the oc expose dc catalog-service --port=8080 command.\n$ oc expose dc catalog-service --port=8080 service/catalog-service exposed Now also expose the service and get the route oc expose service catalog-service --name=catalog-service; oc get route\n$ oc expose service catalog-service --name=catalog-service; oc get route route.route.openshift.io/catalog-service exposed NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD catalog-service catalog-service-testing.apps.c3smonkey.ch catalog-service 8080 None We have to patch the dc imagePullPolicy from IfNotPresent to Always with oc patch command. The default is set to IfNotPresent, but we wish to always trigger a deployment when we tag a new image\n$ oc patch dc/catalog-service -p \\ \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;default-container\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;}]}}}}\u0026#39; Production Environment Deployment The same what we did on the Testing Environment Deployment we also do now on the production environment. But we configure the promotion tag promotePRD in the deployment configuration.\n$ oc project production $ oc create dc catalog-service --image=docker-registry.default.svc:5000/development/catalog-service:promotePRD $ oc patch dc/catalog-service -p \\ \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;default-container\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;}]}}}}\u0026#39; $ oc expose dc catalog-service --port=8080 $ oc expose svc/catalog-service Jenkins Pipeline So now let\u0026rsquo;s create a BuildConfig for the catalaog-service with the following catalog-service-jenkins-pipeline configuration in the Jenkins namespace (project) let\u0026rsquo;s do it with oc create -n jenkins -f https://blog.marcelwidmer.org/openshift-pipeline/catalog-service-pipeline.yaml\n$ oc create -n jenkins -f \\ https://blog.marcelwidmer.org/openshift-pipeline/catalog-service-pipeline.yaml buildconfig.build.openshift.io/catalog-service-pipeline created When you go now in the OpenShift Console in the project Jenkins in the section. Builds/Pipelines you will something like this. Run Jenkins Pipeline Now is time to run the pipeline with the command oc start-build catalog-service-pipeline -n jenkins\n$ oc start-build catalog-service-pipeline -n jenkins build.build.openshift.io/catalog-service-pipeline-1 started After a while you will see something like this. For production deployment we configured our pipeline with a approvable step.\nNow is time to approve the application and hit the After the approve button in the pipeline to deploy to the production namespace.\nWebHooks Now let\u0026rsquo;s creat a WebHook. So when we push something in our catalog-service the pipeline start run. Change first to the jenkins project again with oc project jenkins\n$ oc project jenkins Now using project \u0026#34;jenkins\u0026#34; on server \u0026#34;https://console.c3smonkey.ch:8443\u0026#34; Now check the Buildconfig with oc describe bc/catalog-service-pipeline\n$ oc describe bc/catalog-service-pipeline Name:\tcatalog-service-pipeline Namespace:\tjenkins Created:\t2 hours ago Labels:\tapp=catalog-service-pipeline name=catalog-service-pipeline Annotations:\t\u0026lt;none\u0026gt; Latest Version:\t1 Strategy:\tJenkinsPipeline URL:\thttps://github.com/marzelwidmer/catalog-service.git Ref:\tmaster Jenkinsfile path:\tJenkinsfile Build Run Policy:\tSerial Triggered by:\t\u0026lt;none\u0026gt; Builds History Limit: Successful:\t5 Failed:\t5 Build\tStatus\tDuration\tCreation Time catalog-service-pipeline-1 complete 8m12s 2019-09-05 13:54:18 +0200 CEST Events:\t\u0026lt;none\u0026gt; At the moment we don\u0026rsquo;t have any GitHub Hooks configured.\nBuildConfig Triggers With the following command you can set GitHub WebHook trigger. This will create a secret for us and configured a WebHook in our BuildConfig.\n$ oc set triggers bc/catalog-service-pipeline --from-github When we run again the oc describe bc/catalog-service-pipeline command we will see that we have a bc like below.\n$ oc describe bc/catalog-service-pipeline Name:\tcatalog-service-pipeline Namespace:\tjenkins Created:\t2 hours ago Labels:\tapp=catalog-service-pipeline name=catalog-service-pipeline Annotations:\t\u0026lt;none\u0026gt; Latest Version:\t1 Strategy:\tJenkinsPipeline URL:\thttps://github.com/marzelwidmer/catalog-service.git Ref:\tmaster Jenkinsfile path:\tJenkinsfile Build Run Policy:\tSerial Triggered by:\t\u0026lt;none\u0026gt; Webhook GitHub: URL:\thttps://okd.ch/apis/build.openshift.io/v1/namespaces/jenkins/buildconfigs/catalog-service-pipeline/webhooks/\u0026lt;secret\u0026gt;/github Builds History Limit: Successful:\t5 Failed:\t5 Build\tStatus\tDuration\tCreation Time catalog-service-pipeline-1 complete 8m12s 2019-09-05 13:54:18 +0200 CEST Events:\t\u0026lt;none\u0026gt; Note: The URL we will replace with a secret. This is just an place holder in the URL. https://console.c3smonkey.ch:8443/apis/build.openshift.io/v1/namespaces/jenkins/buildconfigs/catalog-service-pipeline/webhooks//github\nTo grab the \u0026lt;secret\u0026gt; we have to replace in the URL you can call the following command.\n$ oc get bc/catalog-service-pipeline -o json | jq \u0026#39;.spec.triggers[].github.secret\u0026#39; In your GitHub repository, select Add Webhook from Settings ‚Üí Webhooks. Paste the URL output (similar to above) into the Payload URL field.\nHint: SSL Disable (not recommended) if your cluster don\u0026rsquo;t have a valid SSL certificate. SSL verification By default, we verify SSL certificates when delivering payloads.\nReferences:\nhttps://blog.openshift.com/decrease-maven-build-times-openshift-pipelines-using-persistent-volume-claim/ https://github.com/redhat-cop/container-pipelines/tree/master/basic-spring-boot\n","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-08-28-multiple-project-promoting/","summary":"","title":"Promoting Applications Across Environments"},{"content":"Now is time to configure our microservices to send the tracing logs to Jaeger 1. The configuration opentracing.jaeger.http-sender.url in configuration application.yaml file looks like below in the sources.\nopentracing: jaeger: log-spans: true http-sender: url: http://localhost:14268/api/traces The opentracing.jaeger.http-sender.url we are looking for we get form the section Get Route Host in the Jaeger post We will use the ConfigMap approach with the Spring Cloud Kubernetes{:target=\u0026quot;_blank\u0026quot;} starters.\nMaven Update Maven Configuration with Spring Cloud Kubernetes{:target=\u0026quot;_blank\u0026quot;} library.\nDependency Management spring-cloud-dependencies \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; Spring Cloud Version spring-cloud.version \u0026lt;properties\u0026gt; ... \u0026lt;spring-cloud.version\u0026gt;Greenwich.SR3\u0026lt;/spring-cloud.version\u0026gt; \u0026lt;/properties\u0026gt; Dependency spring-cloud-starter-kubernetes-config \u0026lt;!-- Kubernetes --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-kubernetes-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Application Configuration The application may need to detect changes on external property sources and update their internal status to reflect the new configuration. The reload feature of Spring Cloud Kubernetes is able to trigger an application reload when a related ConfigMap changes.\nThis feature is disabled by default and can be enabled using the configuration property spring.cloud.kubernetes.reload.enabled=true in the application.yaml file.\nThe configuration spring.cloud.kubernetes.reload.strategy=restart_context will restart the whole Spring ApplicationContext gracefully.\nspring: cloud: kubernetes: reload: enabled: true strategy: restart_context Configure the management.endpoint.restart.enabled=true\nmanagement: endpoint: restart: enabled: true Configured Service Account - RBAC policy To read the ConfigMap we have to give to the Service Account{:target=\u0026quot;_blank\u0026quot;} in the default namespace access right. This can be done to give just view access oc policy add-role-to-user view system:serviceaccount:development:default\nThe better solution is configure ClusterRole\n‚ö†Ô∏è Avoid no RBAC policy match exception:\n.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://172.30.0.1/api/v1/namespaces/development/pods/order-service-35-wj25f. Message: Forbidden!Configured service account doesnt have access. Service account may have been revoked. pods \u0026ldquo;order-service-35-wj25f\u0026rdquo; is forbidden: User \u0026ldquo;system:serviceaccount:development:default\u0026rdquo; cannot get pods in the namespace \u0026ldquo;development\u0026rdquo;: no RBAC policy matched.\nCreate ClusterRole Additional you can also create a ClusterRole for Spring components let it named spring-roles. Create a file service-account-for-spring-cloud-k8s-access.yaml\nkind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: spring-roles rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group resources: [\u0026#34;pods\u0026#34;,\u0026#34;configmaps\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: allow-spring-to-access-cluster subjects: - kind: ServiceAccount name: default namespace: default roleRef: kind: ClusterRole name: spring-roles apiGroup: rbac.authorization.k8s.io Login in with privileged user oc login -u \u0026lt;privileged user\u0026gt;.\n$ oc apply -f service-account-for-spring-cloud-k8s-access.yaml Now when you check che Cluster Console under Administration/Roles and you search for spring you will find the role.\nDeploy ConfigMap Now is time to create our ConfigMap and apply it in the development namespace for the oder-service You can do it directly in a shell.\n$ echo \u0026#34;apiVersion: v1 kind: ConfigMap metadata: # matches the spring app name as defined in application.yml name: order-service data: # must be named \u0026#39;application.yaml\u0026#39; or be the only key in this config # refer to Spring Cloud Kubernetes Config documentation or source code application.yaml: | opentracing: jaeger: http-sender: url: http://jaeger-collector-jaeger.apps.c3smonkey.ch/api/traces\u0026#34; | oc apply -f - Better is you create a ConfigMap file and then you use the apply command.\n$ oc apply -f deployments/configmap.yaml When you hit the service again you will see some traces in the Jaeger now.\n$ for x in (seq 50); http \u0026#34;http://order-service-development.apps.c3smonkey.ch/api/v1/orders/random\u0026#34;; end jaegertracing.io\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-09-18-spring-boot-k8s-configmap/","summary":"","title":"Spring Boot Kubernetes ConfigMap"},{"content":"Spring Cloud Kubernetes Ribbon provide a mechanism to perform a client side load-balancing who is needed in a microservice architecture to allocate a list of all pods where our service is running (replicated)\nThis mechanism can automatically discover and reach all the endpoints of a specific service, and subsequently, it populates a Ribbon ServerList with information about the endpoints.\nLet\u0026rsquo;s start by adding the spring-cloud-starter-kubernetes-ribbon dependency to our pom.xml file:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-kubernetes-ribbon\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; When the list of the endpoints is populated, the K8s client will search the registered endpoints living in the current namespace\nWe also need to enable the ribbon client in the application.yaml:\nribbon.http.client.enabled=true Now add the @EnableDiscoveryClient annotation in the Spring Boot application.\n@EnableDiscoveryClient @SpringBootApplication class CatalogServiceClient As next we also add the @LoadBalanced annotation.\nclass OrderServiceConfiguration { @Bean @LoadBalanced fun webClientBuilder() = WebClient.builder() } Now we can use now the service name http://catalog-service to call other services.\n@Service class CatalogServiceClient(private val webClientBuilder: WebClient.Builder) { companion object { val CATALOG_SERVICE_URL = \u0026#34;http://catalog-service/api/v1/animals/random\u0026#34; } fun getRandomAnimalNames(): Flux\u0026lt;String\u0026gt; { return this.webClientBuilder .baseUrl(CATALOG_SERVICE_URL).build() .get() .accept(MediaType.APPLICATION_JSON) .retrieve().bodyToFlux(String::class.java) .log() } } ","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-09-21-spring-boot-k8s-ribbon-discovery/","summary":"","title":"Spring Boot Kubernetes Discovery"},{"content":"Setup Deployment $ oc new-project development --display-nam e=\u0026#34;Development Environment\u0026#34; Deploy application with the maven.fabric8.io plugin in development stage from local machine.\n$ ./mvnw fabric8:deploy -Dfabric8.namespace=development $ oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n development Create testing project and setup the roles.\n$ oc new-project testing --display-name=\u0026#34;Testing Environment\u0026#34; $ oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n testing $ oc policy add-role-to-group system:image-puller system:serviceaccounts:testing \\ -n development Create DeploymentConfiguration in testing stage.\n$ oc create dc customer-service --image=docker-registry.default.svc:5000/development/customer-service:promoteQA -n testing $ oc patch dc/customer-service -p \\ \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;default-container\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;}]}}}}\u0026#39; -n testing $ oc expose dc customer-service -n testing --port=8080 $ oc expose svc/customer-service -n testing Create production project and setup the roles.\n$ oc new-project production --display-name=\u0026#34;Production Environment\u0026#34; $ oc policy add-role-to-user edit system:serviceaccount:jenkins:jenkins -n production $ oc policy add-role-to-group system:image-puller system:serviceaccounts:production \\ -n development Create DeploymentConfiguration in production stage.\n$ oc create dc customer-service --image=docker-registry.default.svc:5000/development/customer-service:promotePROD -n production $ oc patch dc/customer-service -p \\ \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;template\u0026#34;:{\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;default-container\u0026#34;,\u0026#34;imagePullPolicy\u0026#34;:\u0026#34;Always\u0026#34;}]}}}}\u0026#39; -n production $ oc expose dc customer-service -n production --port=8080 $ oc expose svc/customer-service -n production Jenkins Pipeline Let\u0026rsquo;s creat a Jenkins Pipeline for the customer-service in the project jenkins.\n$ oc create -n jenkins -f \\ https://blog.marcelwidmer.org/semantic-release-delivery-pipeline/deploy/customer-service-pipeline.yaml WebHooks How we can create a GitHub WebHook for a public Git repository take a look at the following post there we created already a\nWebHook for the catalog-service but here some oc commands for the customer-service.\n$ oc set triggers bc/customer-service-pipeline --from-github -n jenkins Grab the Secret.\n$ oc get bc/customer-service-pipeline -n jenkins -o json | jq \u0026#39;.spec.triggers[].github.secret\u0026#39; Grab Webhook GitHub URL.\n$ oc describe bc/customer-service-pipeline -n jenkins Private Repository access with secrets Create a generic secret link this secret with the builder. Annotate and label it for the Jenkins sync PlugIn. And finally update the bc/customer-service-pipeline with this secret. First you have to create an AccessToken in your GitHub Tokens Settings let it named like openshift-source-builder add repo and user access because this token will be used for Semantic Release\n$ oc create secret generic ci-user-at-github \\ --from-literal=username=machineuser \\ --from-literal=password=\u0026lt;accesstoken\u0026gt; \\ --type=kubernetes.io/basic-auth \\ -n jenkins $ oc secrets link builder ci-user-at-github \\ -n jenkins $ oc annotate secret/ci-user-at-github \\ \u0026#39;build.openshift.io/source-secret-match-uri-1=https://github.com/marzelwidmer/*\u0026#39; \\ -n jenkins $ oc label secret ci-user-at-github credential.sync.jenkins.openshift.io=true \\ -n jenkins $ oc set build-secret bc/customer-service-pipeline ci-user-at-github --source When you check now the Jenkins you will see the ci-user-at-github under credentials https://\u0026lt;jenkins-url\u0026gt;/credentials/\nYou will also find the a secret ci-user-at-github in the jenkins project in the OpenShift console.\nSemantic Release Jenkins Pipeline First I want say the inspiration I get from the semantic-release automates the whole package release workflow including: determining the next version number, generating the release notes and publishing the package.\nüòé This removes the immediate connection between human emotions and version numbers, strictly following the Semantic Versioning specification.\nIn the case I don\u0026rsquo;t found any Maven PlugIn who works in my setup out-of-the-box and I am running here in a Maven Slave and don\u0026rsquo;t want create a Maven-Node Slave I chose to follow a setup with just Git commands and a combination with the jgitver-maven-plugin.\nAfter pushing some code in the customer-service repository the Jenkins pipeline start run. It will be tag the source repository if needed based on the commit message inspired on the follow commit message format.\n‚ö†Ô∏è Commit Message Format: Current version 1.0.0 will change the version like:\nMajor version (2.0.0) üëâüèº breaking:|major:|BREAKING CHANGE:\nMinor version (1.1.0) üëâüèº feature:|minor:|feat:\nPatch version (1.0.1) üëâüèº fix:|patch:|docs:|style:|refactor:|perf:|test:|chore:\nIt will also create the image tags if needed.\nTake also a look at the Jenkins BlueOcean pipeline.\nOr at the deployed customer-service.\nCustomer Service Swagger The changelog you can find under Changelog\nI know the above pipeline is a bit chatty because of this I explain my steps here a bit compromised in some pipeline steps. Ok let\u0026rsquo;s create a Jenkinsfile in your project repository I prefer to put the Jenkinsfile in a folder jenkins.\nFirst we configure a Maven Slave for this we configure the agent node with the laben maven because our porject is a Maven project.\npipeline { // Agent Maven agent { node { label \u0026#39;maven\u0026#39; } } } Now lets define some environment variables we can parameterize the pipeline also for other projects later.\n// Environment environment { REPOSITORY = \u0026#34;github.com/marzelwidmer/customer-service.git\u0026#34; BRANCHES = \u0026#34;[[name: \u0026#39;*/master\u0026#39;]]\u0026#34; GIT_TAG_MESSAGE = \u0026#39;ci-release-bot\u0026#39; GIT_TAG_USER_EMAIL = \u0026#34;jenkins@c3smonkey.ch\u0026#34; GIT_TAG_USER_NAME = \u0026#34;Jenkins\u0026#34; DEV_ENVIRONMENT = \u0026#39;development\u0026#39; TEST_ENVIRONMENT = \u0026#39;testing\u0026#39; PROD_ENVIRONMENT = \u0026#39;production\u0026#39; APP_NAME = \u0026#39;customer-service\u0026#39; } (1) The following step will first check the last GIT_COMMIT ID and download a ci-semver.sh script who is hosted in a other central repository who can used for central CI/CD scripts across different project Git repositories.\n(2) This shell script will then check the Git history if there a commit message who match our Semantic Version naming convention to compute the next version.\n(3) The next command will then call the Maven build and test lifecycle and use the jgitver plugin to pass the version we get before from the ci-semver.sh script.\nüí° jgitver: https://www.youtube.com/watch?v=mQmH_Ws9GFI\n(4) This section will store our JUnit test results in the Jenkins.\n(5) Tag Git repository if needed.\n// Stages stages { // Setup stage(\u0026#39;build and test application\u0026#39;) { steps { script { // Last Git commit LAST_GIT_COMMIT = sh( script: \u0026#39;git --no-pager show -s --format=\\\u0026#39;%Cblue %h %Creset %s %Cgreen %an %Creset (%ae)\\\u0026#39;\u0026#39;, returnStdout: true ).trim() echo \u0026#34;Last Git commit: ${LAST_GIT_COMMIT}\u0026#34; // Download script (1) sh \u0026#39;\u0026#39;\u0026#39; echo download ci-semver.sh script from remote repository curl https://raw.githubusercontent.com/marzelwidmer/git-semantic-commits/master/ci-semver.sh --output ./jenkins/ci-semver.sh chmod +x ./jenkins/ci-semver.sh \u0026#39;\u0026#39;\u0026#39; // Compute next version (2) NEXT_VERSION = sh( script: \u0026#34;./jenkins/ci-semver.sh\u0026#34;, returnStdout: true ).trim() echo \u0026#34;NEXT_VERSION : ${NEXT_VERSION}\u0026#34; withCredentials([usernamePassword(credentialsId: \u0026#39;jenkins-ci-user-at-github\u0026#39;, usernameVariable: \u0026#39;USERNAME\u0026#39;, passwordVariable: \u0026#39;TOKEN\u0026#39;)]) { git(branches: \u0026#34;$BRANCHES\u0026#34;, changelog: true, url: \u0026#34;https://$TOKEN:x-oauth-basic@$REPOSITORY\u0026#34;) } // Maven build and test (3) sh(\u0026#34;\u0026#34;\u0026#34; echo next version will be $NEXT_VERSION ./mvnw validate ./mvnw validate -Djgitver.use-version=$NEXT_VERSION ./mvnw package -DskipTests -Djgitver.use-version=$NEXT_VERSION echo \u0026#34;run tests\u0026#34; sh \u0026#39;./mvnw test\u0026#39; \u0026#34;\u0026#34;\u0026#34;) // Store Junit results (4) post { always { junit \u0026#39;target/surefire-reports/*.xml\u0026#39; } } // Tag Git Repository (5) echo \u0026#34;Create tag with version: ${NEXT_VERSION}\u0026#34; sh(\u0026#34;\u0026#34;\u0026#34; echo set git config for tagging git config --global user.email \u0026#39;${GIT_TAG_USER_EMAIL}\u0026#39; git config --global user.name \u0026#39;${GIT_TAG_USER_NAME}\u0026#39; if [ \\$(git tag -l $NEXT_VERSION) ]; then echo tag exist already else git tag -a -m \u0026#39;${GIT_TAG_MESSAGE}\u0026#39; ${NEXT_VERSION} git push --follow-tags fi \u0026#34;\u0026#34;\u0026#34;) } } } } References and inspiration:\nJenkins Client Plugin Best Practices for Managing Docker Versions CI/CD - A/B - OpenShift - Jenkins GitLab Private Repository Personal Access Tokens - Private Repository Pipelines with git tags jgitver - goal jgitver-maven-plugin\nSemantic commits for git git-semantic-commits Jenkins - semantic-versioning-plugin Jenkins - Git+Parameter+Plugin wilsonmar - jenkins2-pipeline Get Jenkins GDSL working with IntelliJ IDEA\n","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-09-08-semantic-release-delivery-pipeline/","summary":"","title":"Semantic Release Delivery Pipeline"},{"content":"As on-the-ground microservice practitioners are quickly realizing, the majority of operational problems that arise when moving to a distributed architecture are ultimately grounded in two areas: networking and observability. It is simply an orders of magnitude larger problem to network and debug a set of intertwined distributed services versus a single monolithic application. Jaeger Jaeger is a open source, end-to-end distributed tracing Monitor and troubleshoot transactions in complex distributed systems. CNCF Webinar Intro Jaeger{:target=\u0026quot;_blank\u0026quot;}\nNOTE: Installing the operator on OKD Openshift jaegertracing.io{:target=\u0026quot;_blank\u0026quot;}\nJeager All-In-One with OpenShift Template Login in with your developer user and create a jaeger project.\n$ oc new-project jaeger --display-name=\u0026#34;Distributed Tracing System\u0026#34; Install Jaeger on OpenShift to collect the traces\n$ oc process -f https://blog.marcelwidmer.org/img/2019/jaeger/deploy/jaeger-all-in-one-template.yml \\ | oc create -f - üí° Template based on: jaegertracing/jaeger-openshift{:target=\u0026quot;_blank\u0026quot;}\nCreate Route Create a route to access the Jaeger collector\n$ oc expose service jaeger-collector --port=14268 -n jaeger Get Route Host Get the route address\n$ oc get route/jaeger-collector -n jaeger -o json | jq \u0026#39;.spec.host\u0026#39; \u0026#34;jaeger-collector-jaeger.apps.c3smonkey.ch\u0026#34; This address we will configure in our Spring Boot k8s{:target=\u0026quot;_blank\u0026quot;} Application.\nJaeger Query UI Installing the Operator on OpenShift Step-by-Step with separate operator Login in with privileged user oc login -u \u0026lt;privileged user\u0026gt; and create a jaeger project to install the operator. This creates the namespace used by default in the deployment files. If you want to install the Jaeger operator in a different namespace, you must edit the deployment files to change jaeger to the desired namespace value.\n$ oc new-project jaeger --display-name=\u0026#34;Distributed Tracing System\u0026#34; Deploy CustomResourceDefinition{:target=\u0026quot;_blank\u0026quot;} for the apiVersion: jaegertracing.io/v1 ServiceAccount{:target=\u0026quot;_blank\u0026quot;} ClusterRole{:target=\u0026quot;_blank\u0026quot;} ClusterRoleBinding{:target=\u0026quot;_blank\u0026quot;} Operator{:target=\u0026quot;_blank\u0026quot;}\n$ oc create -f \\ https://blog.marcelwidmer.org/img/2019/jaeger/deploy/jaegertracing_v1_jaeger_crd.yaml $ oc create -f \\ https://blog.marcelwidmer.org/img/2019/jaeger/deploy/service_account.yaml $ oc create -f \\ https://blog.marcelwidmer.org/img/2019/jaeger/deploy/role.yaml $ oc create -f \\ https://blog.marcelwidmer.org/img/2019/jaeger/deploy/role_binding.yaml $ oc create -f \\ https://blog.marcelwidmer.org/img/2019/jaeger/deploy/operator.yaml Grant the role jaeger-operator to users who should be able to install individual Jaeger instances. The following example creates a role binding allowing the user developer to create Jaeger instances:\n$ oc create \\ rolebinding developer-jaeger-operator \\ --role=jaeger-operator \\ --user=developer After the role is granted, switch back to a non-privileged user.\nQuick Start - Deploying the AllInOne image The simplest possible way to create a Jaeger instance is by creating a YAML file like the following example. This will install the default AllInOne strategy, which deploys the ‚Äúall-in-one‚Äù image (agent, collector, query, ingestor, Jaeger UI) in a single pod, using in-memory storage by default.\n‚ö†Ô∏è Production installation: For Production installation take a look at the official production-strategy documentation.\nLogin in with privileged user oc login -u \u0026lt;privileged user\u0026gt;\n$ echo \u0026#34;apiVersion: jaegertracing.io/v1 kind: Jaeger metadata: name: jaeger-collector\u0026#34; | oc create -f - To get the pod name, query for the pods belonging to the jaeger-collector Jaeger instance:\n$ oc get jaegers NAME AGE jaeger-collector 10s To get the routes:\n$ oc get route NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD jaeger-collector jaeger-collector-jaeger.apps.c3smonkey.ch jaeger-collector-query \u0026lt;all\u0026gt; reencrypt None Jaeger Query UI Open the Browser and login with your OpenShift developer credentials to the Jaeger UI{:target=\u0026quot;_blank\u0026quot;} First time you logged in you have to accept the Authorize Access\n","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-09-01-jaeger/","summary":"","title":"Jaeger - Distributed Tracing System"},{"content":"Inspiration from [Adding an SSL certificate to OKD - Part 2 of Installation of OKD 3.10 from start to finish]https://www.youtube.com/watch?v=S7HoJ09oYn0\u0026amp;feature=youtu.be){:target=\u0026quot;_blank\u0026quot;}\nCheck Certificate Let`s check first if there a certificate already for our domain https://crt.sh/?q=c3smonkey.ch\nCheck EPEL Reposittory Check if the epel.repo is enabled.\n$ vi /etc/yum.repos.d/epel.repo [epel] name=Extra Packages for Enterprise Linux 7 - $basearch #baseurl=http://download.fedoraproject.org/pub/epel/7/$basearch metalink=https://mirrors.fedoraproject.org/metalink?repo=epel-7\u0026amp;arch=$basearch failovermethod=priority enabled=0 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 change enabled=0 to enabled=1 and quite and save the file with :qw\nLogin to the Cluster Login to the Cluster with the oc login command. Check if you are in the project default other wise change to the default project with oc project default\n$ [root@c3smonkey ~]# oc login Authentication required for https://console.c3smonkey.ch:8443 (openshift) Username: monkey Password: Login successful. You have access to the following projects and can switch between them with \u0026#39;oc project \u0026lt;projectname\u0026gt;\u0026#39;: * default development jenkins Verify if you are in the default project namespace.\n$ [root@c3smonkey ~]# oc project Using project \u0026#34;default\u0026#34; on server \u0026#34;https://console.c3smonkey.ch:8443\u0026#34;. To create our certificate with certbot we take the temporary webserver approach. For this we have to scale done one pod on the cluster who are running already on the port 80.\nSearch Router and scale it down. With the oc get dc we can check if the router is running.\n$ [root@c3smonkey ~]# oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY docker-registry 1 1 1 config registry-console 1 1 1 config router 1 1 1 config Now we can scale it down with oc scale --replicas=0 dc router\n$ oc scale --replicas=0 dc router deploymentconfig.apps.openshift.io/router scaled Lets verify if the router not running before we install and run the certbot with the oc get dc command like before.\n$ [root@c3smonkey ~]# oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY docker-registry 1 1 1 config registry-console 1 1 1 config router 1 0 0 config Lets install the certbot now with the yum install certbot command.\n$ [root@c3smonkey ~]# yum install certbot Check if the DNS server is setup is correct. Important * CNAME is pointing to apps.console We need this because whne we deploy new application this will create a URL under apps and the namespace.\n* 420 IN CNAME apps.console @ 1800 IN A 95.216.193.150 apps.console 300 IN A 95.216.193.150 console 300 IN A 95.216.193.150 Here you can verify the domain names dnschecker.org Ok let`s create some certificate for the following domain names\nc3smonkey.ch console.c3smonkey.ch jenkins-jenkins.apps.c3smonkey.ch grafana-openshift-monitoring.apps.c3smonkey.ch apps.c3smonkey.ch hawkular-metrics.apps.c3smonkey.ch $ [root@c3smonkey ~]# certbot certonly --server https://acme-v02.api.letsencrypt.org/directory \\ --standalone \\ -d console.c3smonkey.ch \\ -d jenkins-jenkins.apps.c3smonkey.ch \\ -d c3smonkey.ch \\ -d grafana-openshift-monitoring.apps.c3smonkey.ch \\ -d apps.c3smonkey.ch \\ -d hawkular-metrics.apps.c3smonkey.ch After this command you see something like this\nThe certificate are located under /etc/letsencrypt/live/console.c3smonkey.ch/\n$ root@c3smonkey installcentos]# ls -lisa /etc/letsencrypt/live/console.c3smonkey.ch/ total 12 1258906 4 drwxr-xr-x. 2 root root 4096 Aug 26 21:06 . 1258902 4 drwx------. 3 root root 4096 Aug 26 21:06 .. 1258915 4 -rw-r--r--. 1 root root 692 Aug 26 21:06 README 1258907 0 lrwxrwxrwx. 1 root root 44 Aug 26 21:06 cert.pem -\u0026gt; ../../archive/console.c3smonkey.ch/cert1.pem 1258909 0 lrwxrwxrwx. 1 root root 45 Aug 26 21:06 chain.pem -\u0026gt; ../../archive/console.c3smonkey.ch/chain1.pem 1258910 0 lrwxrwxrwx. 1 root root 49 Aug 26 21:06 fullchain.pem -\u0026gt; ../../archive/console.c3smonkey.ch/fullchain1.pem 1258908 0 lrwxrwxrwx. 1 root root 47 Aug 26 21:06 privkey.pem -\u0026gt; ../../archive/console.c3smonkey.ch/privkey1.pem Now we can scale up our route with oc scale --replicas=1 dc router and verify it with oc get dc\n$ [root@c3smonkey]# oc scale --replicas=1 dc router deploymentconfig.apps.openshift.io/router scaled [root@c3smonkey]# oc get dc NAME REVISION DESIRED CURRENT TRIGGERED BY docker-registry 1 1 1 config registry-console 1 1 1 config router 1 1 1 config [root@c3smonkey installcentos]# Now we have to patch the installation inventory.ini file for this we will take the vi We will add the following section on the bottom of the file.\nopenshift_master_overwrite_named_certificates=true openshift_master_named_certificates=[{\u0026#34;certfile\u0026#34;: \u0026#34;/etc/letsencrypt/live/console.c3smonkey.ch/cert.pem\u0026#34;, \u0026#34;keyfile\u0026#34;: \u0026#34;/etc/letsencrypt/live/console.c3smonkey.ch/privkey.pem\u0026#34;,\u0026#34;names\u0026#34;: [\u0026#34;c3smonkey.ch\u0026#34;, \u0026#34;console.c3smonkey.ch\u0026#34;, \u0026#34;jenkins-jenkins.apps.c3smonkey.ch\u0026#34;, \u0026#34;grafana-openshift-monitoring.apps.c3smonkey.ch\u0026#34;, \u0026#34;apps.c3smonkey.ch\u0026#34;, \u0026#34;hawkular-metrics.apps.c3smonkey.ch\u0026#34;] }] The meaning of this configuration is basically that we override the master cluster certificate with the flag true in openshift_master_overwrite_named_certificates=true and add all named certificates we created with the certbot command from before.\nc3smonkey.ch console.c3smonkey.ch jenkins-jenkins.apps.c3smonkey.ch grafana-openshift-monitoring.apps.c3smonkey.ch apps.c3smonkey.ch hawkular-metrics.apps.c3smonkey.ch let us change the directory first and jump in the installation directory cd installcentos\n$ [root@c3smonkey ~]# cd installcentos/ Now let\u0026rsquo;s apply two ansible script but for this lets search the ansible scripts we need with cat install-openshift.sh | grep ansible\n$ [root@c3smonkey installcentos]# cat install-openshift.sh | grep ansible curl -o ansible.rpm https://releases.ansible.com/ansible/rpm/release/epel-7-x86_64/ansible-2.6.5-1.el7.ans.noarch.rpm yum -y --enablerepo=epel install ansible.rpm [ ! -d openshift-ansible ] \u0026amp;\u0026amp; git clone https://github.com/openshift/openshift-ansible.git # cd openshift-ansible \u0026amp;\u0026amp; git fetch \u0026amp;\u0026amp; git checkout release-${VERSION} \u0026amp;\u0026amp; git checkout e7f05191a1 \u0026amp;\u0026amp; cd .. cd openshift-ansible \u0026amp;\u0026amp; git fetch \u0026amp;\u0026amp; git checkout release-${VERSION} \u0026amp;\u0026amp; cd .. # https://github.com/openshift/openshift-ansible/issues/11069 ansible-playbook -i inventory.ini openshift-ansible/playbooks/prerequisites.yml ansible-playbook -i inventory.ini openshift-ansible/playbooks/deploy_cluster.yml The first one we will apply is the prerequisites playbook to check if everything is still good.\n$ [root@c3smonkey installcentos]# ansible-playbook -i inventory.ini openshift-ansible/playbooks/prerequisites.yml Lets play the second playbook who will replace the certificate we just created.\n$ [root@c3smonkey installcentos]# ansible-playbook -i inventory.ini openshift-ansible/playbooks/deploy_cluster.yml When it stuck and the API server don\u0026rsquo;t come back up just rerun the script. or run playbooks/openshift-master/config.yml\n$ [root@c3smonkey installcentos]# ansible-playbook -i inventory.ini openshift-ansible/playbooks/openshift-master/config.yml This will take a time dependence of your host. Now reboot the cluster with shutdown -r now\n$ [root@c3smonkey installcentos]# shutdown -r now Let us check the certificate https://crt.sh/?q=c3smonkey.ch Open a new tab and open the https://console.c3smonkey.ch:8443/ to check if the certificate are installed successfully.\nBackup Certificates $ scp root@c3smonkey.ch:/etc/letsencrypt/live/console.c3smonkey.ch/\\*.pem ~/dev/c3smonkey/hetzner-okd-ansible/cert/ Restore Certificates First create the following folder structure /etc/letsencrypt/live/console.c3smonkey.ch/ on the remote host.\n$ [root@c3smonkey ~]# mkdir -p /etc/letsencrypt/live/console.c3smonkey.ch/ Lets copy the files cert.pem chain.pem fullchain.pem privkey.pemto the folder from your local backup.\n$ scp ~/dev/c3smonkey/hetzner-okd-ansible/cert/cert.pem root@c3smonkey.ch:/etc/letsencrypt/live/console.c3smonkey.ch/ $ scp ~/dev/c3smonkey/hetzner-okd-ansible/cert/chain.pem root@c3smonkey.ch:/etc/letsencrypt/live/console.c3smonkey.ch/ $ scp ~/dev/c3smonkey/hetzner-okd-ansible/cert/fullchain.pem root@c3smonkey.ch:/etc/letsencrypt/live/console.c3smonkey.ch/ $ scp ~/dev/c3smonkey/hetzner-okd-ansible/cert/privkey.pem root@c3smonkey.ch:/etc/letsencrypt/live/console.c3smonkey.ch/ ","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-08-20-certbot-letsencrypt/","summary":"","title":"Certbot Let`s Encrypt"},{"content":"Inspiration from Installation of OKD 3.10 from start to finish\nCreate a Hetzner VM with the CLI https://github.com/hetznercloud/cli\nhcloud (Hetzner CLI) Let\u0026rsquo;s check first some hcloud command that we can use later to create a VM with the right size and in the Datacenter we want.\nhcloud server create --name \u0026lt;YOUR_DOMAIN\u0026gt; --type \u0026lt;SERVER-TYPE\u0026gt; --image \u0026lt;IMAGE\u0026gt; --ssh-key \u0026lt;YOUR_HETZNER_SSH_KEY\u0026gt; --datacenter \u0026lt;DATACENTER\u0026gt; Server Type hcloud server-type list ID NAME CORES MEMORY DISK STORAGE TYPE 1 cx11 1 2.0 GB 20 GB local 2 cx11-ceph 1 2.0 GB 20 GB network 3 cx21 2 4.0 GB 40 GB local 4 cx21-ceph 2 4.0 GB 40 GB network 5 cx31 2 8.0 GB 80 GB local 6 cx31-ceph 2 8.0 GB 80 GB network 7 cx41 4 16.0 GB 160 GB local 8 cx41-ceph 4 16.0 GB 160 GB network 9 cx51 8 32.0 GB 240 GB local 10 cx51-ceph 8 32.0 GB 240 GB network 11 ccx11 2 8.0 GB 80 GB local 12 ccx21 4 16.0 GB 160 GB local 13 ccx31 8 32.0 GB 240 GB local 14 ccx41 16 64.0 GB 360 GB local 15 ccx51 32 128.0 GB 600 GB local 22 cpx11 2 2.0 GB 40 GB local 23 cpx21 3 4.0 GB 80 GB local 24 cpx31 4 8.0 GB 160 GB local 25 cpx41 8 16.0 GB 240 GB local 26 cpx51 16 32.0 GB 360 GB local Image hcloud image list ID TYPE NAME DESCRIPTION IMAGE SIZE DISK SIZE CREATED 1 system ubuntu-16.04 Ubuntu 16.04 - 5 GB 2 years ago 2 system debian-9 Debian 9 - 5 GB 2 years ago 3 system centos-7 CentOS 7 - 5 GB 2 years ago 168855 system ubuntu-18.04 Ubuntu 18.04 - 5 GB 2 years ago 5924233 system debian-10 Debian 10 - 5 GB 9 months ago 8356453 system centos-8 CentOS 8 - 5 GB 6 months ago 9032843 system fedora-31 Fedora 31 - 5 GB 5 months ago 15512617 system ubuntu-20.04 Ubuntu 20.04 - 5 GB 2 days ago Datacenter hcloud datacenter list ID NAME DESCRIPTION LOCATION 2 nbg1-dc3 Nuremberg 1 DC 3 nbg1 3 hel1-dc2 Helsinki 1 DC 2 hel1 4 fsn1-dc14 Falkenstein 1 DC14 fsn1 Create Server Let\u0026rsquo;s create a Server cx41 with CentOS centos-7 (because of some issues with centos-8) on a data center in Nuremberg nbg1-dc3. When the Server is we will get a Public IP 116.203.16.100.\nhcloud server create --name keepcalm.ch --type cx41 --image centos-8 --ssh-key ~/.ssh/id_rsa_hetzner.pub --datacenter nbg1-dc3 6s [=====================================] 100.00% Waiting for server 5577861 to have started ... done Server 5577861 created IPv4: 116.203.16.100 Rebuild Server if you have already one and want it just rest and re-install the cluster you can user the following command.\nhcloud server rebuild keepcalm.ch --image centos-7 Delete Server hcloud server delete keepcalm.ch Configure DNS Configure now the following DNS entries in your DNS provider with the IP from Hetzner in my case I have gandi.net.\n@ 10800 IN SOA ns1.gandi.net. hostmaster.gandi.net. 1585063961 10800 3600 604800 10800 * 420 IN CNAME apps.console @ 1800 IN A 116.203.16.100 apps.console 300 IN A 116.203.16.100 console 300 IN A 116.203.16.100 Install OKD Now you can connect with SSH to your VM\nssh -i ~/.ssh/id_rsa_hetzner root@keepcalm.ch The authenticity of host \u0026#39;keepcalm.ch (116.203.16.100)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:mc7itrGp4777okbKnKDKPDlQwkMi0e4awyh6cfssNXM. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;keepcalm.ch,116.203.16.100\u0026#39; (ECDSA) to the list of known hosts. Last failed login: Sun Apr 26 12:02:57 CEST 2020 from 51.77.212.235 on ssh:notty There were 3 failed login attempts since the last successful login. [root@keepcalm ~]# Update System and install GIT yum -y update \u0026amp;\u0026amp; yum -y install git Clone Git Repo git clone https://github.com/marzelwidmer/installcentos.git Update user-custom-exports script Update user-custom-exports.sh script with your domain specific settings.\ncd installcentos \u0026amp;\u0026amp; vi user-custom-exports.sh #!/bin/bash export DOMAIN=\u0026#34;keepcalm.ch\u0026#34; export USERNAME=\u0026#34;admin\u0026#34; export PASSWORD=\u0026#34;password\u0026#34; export MAIL=\u0026#34;marzelwidmer@gmail.com\u0026#34; export SCRIPT_REPO=\u0026#34;\u0026#34; export IP=\u0026#34;\u0026#34; export DISK=\u0026#34;\u0026#34; Load user-custom-exports script Execute the shell script to load the variables\n. user-custom-exports.sh Start install script Now we can execute the installation script and press enter till the question Do you wish to enable HTTPS with Let``s Encrypt There we choose 1) Yes.\nroot@keepcalm installcentos]# ./install-openshift.sh Domain to use: (keepcalm.ch): Username: (admin): Password: (password): OpenShift Version: (3.11): IP: (0): API Port: (8443): Do you wish to enable HTTPS with Let\u0026#39;s Encrypt? Warnings: Let\u0026#39;s Encrypt only works if the IP is using publicly accessible IP and custom certificates. This feature doesn\u0026#39;t work with OpenShift CLI for now. 1) Yes 2) No The following questions we answer with yes.\nComplete! Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator manual, Installer None Starting new HTTPS connection (1): acme-v02.api.letsencrypt.org - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Would you be willing to share your email address with the Electronic Frontier Foundation, a founding partner of the Let\u0026#39;s Encrypt project and the non-profit organization that develops Certbot? We\u0026#39;d like to send you email about our work encrypting the web, EFF news, campaigns, and ways to support digital freedom. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (Y)es/(N)o: yes Starting new HTTPS connection (1): supporters.eff.org Obtaining a new certificate Performing the following challenges: dns-01 challenge for apps.keepcalm.ch dns-01 challenge for keepcalm.ch dns-01 challenge for keepcalm.ch - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - NOTE: The IP of this machine will be publicly logged as having requested this certificate. If you\u0026#39;re running certbot in manual mode on a machine that is not your server, please ensure you\u0026#39;re okay with that. Are you OK with your IP being logged? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (Y)es/(N)o: yes ‚ö†Ô∏è Important Steps: Be sure you update the DNS TXT records with the prompted values\nThe following values you have to add in you DNS. So let\u0026rsquo;s open again the DNS Admin console from your provider.\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Please deploy a DNS TXT record under the name _acme-challenge.apps.keepcalm.ch with the following value: xj9uF017asqGXhuS1VPhA3idwmUq__UgUTscTMYgbxI Before continuing, verify the record is deployed. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue In my case something like so :\nYou can check the DNS with https://dnschecker.org/ Check with host command.\nhost -t txt _acme-challenge.apps.keepcalm.ch _acme-challenge.apps.keepcalm.ch descriptive text \u0026#34;xj9uF017asqGXhuS1VPhA3idwmUq__UgUTscTMYgbxI\u0026#34; Your DNS will look like this:\n@ 10800 IN SOA ns1.gandi.net. hostmaster.gandi.net. 1587897751 10800 3600 604800 10800 * 420 IN CNAME apps.console @ 1800 IN A 116.203.16.100 _acme-challenge 300 IN TXT \u0026#34;H2S720yowtzUBVKq3EbH6W8rmgg7qzF0EEVF_IZOz9c\u0026#34; _acme-challenge 300 IN TXT \u0026#34;dpMeonbQDeHkbJopDOuXw1j4NtFHXzEvr8CGutiRX8w\u0026#34; _acme-challenge.apps 300 IN TXT \u0026#34;xj9uF017asqGXhuS1VPhA3idwmUq__UgUTscTMYgbxI\u0026#34; apps.console 300 IN A 116.203.16.100 console 300 IN A 116.203.16.100 Before continuing, verify the record is deployed. (This must be set up in addition to the previous challenges; do not remove, replace, or undo the previous challenge tasks yet. Note that you might be asked to create multiple distinct TXT records with the same name. This is permitted by DNS standards.) - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue Waiting for verification... Resetting dropped connection: acme-v02.api.letsencrypt.org Cleaning up challenges IMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/keepcalm.ch/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/keepcalm.ch/privkey.pem Your cert will expire on 2020-07-25. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \u0026#34;certbot renew\u0026#34; - Your account credentials have been saved in your Certbot configuration directory at /etc/letsencrypt. You should make a secure backup of this folder now. This configuration directory will also contain certificates and private keys obtained by Certbot so making regular backups of this folder is ideal. - If you like Certbot, please consider supporting our work by: Donating to ISRG / Let\u0026#39;s Encrypt: https://letsencrypt.org/donate Donating to EFF: https://eff.org/donate-le Now you have to wait some minutes till the installation is finished.\ncreated volume 199 persistentvolume/vol200 created created volume 200 ****** * Your console is https://console.keepcalm.ch:8443 * Your username is admin * Your password is password * * Login using: * $ oc login -u admin -p password https://console.keepcalm.ch:8443/ ****** Login successful. You have access to the following projects and can switch between them with \u0026#39;oc project \u0026lt;projectname\u0026gt;\u0026#39;: * default kube-public kube-service-catalog kube-system management-infra openshift openshift-console openshift-infra openshift-logging openshift-metrics-server openshift-monitoring openshift-node openshift-sdn openshift-template-service-broker openshift-web-console Using project \u0026#34;default\u0026#34;. [root@keepcalm installcentos]# After the installation is finish you can on : https://console.keepcalm.ch:8443/console/catalog You have now a OKD instance with Let\u0026rsquo;s Encrypt.\nRenewal Let\u0026rsquo;s Encrypt There is a crontab with renew command.\ncrontab -l @weekly certbot renew --pre-hook=\u0026#34;oc scale --replicas=0 dc router\u0026#34; --post-hook=\u0026#34;oc scale --replicas=1 dc router\u0026#34; Check Certificate certbot certificates Saving debug log to /var/log/letsencrypt/letsencrypt.log - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Found the following certs: Certificate Name: c3smonkey.ch Serial Number: 4657db8105e7b58c87e5f69ff91a84a4178 Domains: c3smonkey.ch *.apps.c3smonkey.ch *.c3smonkey.ch Expiry Date: 2020-10-22 07:16:07+00:00 (VALID: 89 days) Certificate Path: /etc/letsencrypt/live/c3smonkey.ch/fullchain.pem Private Key Path: /etc/letsencrypt/live/c3smonkey.ch/privkey.pem - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ","permalink":"https://blog.marcelwidmer.org/blog/2019/2019-01-13-hetzner-okd/","summary":"","title":"Install OKD on Hetzner Cloud"},{"content":"","permalink":"https://blog.marcelwidmer.org/contact/","summary":"How can I help you?","title":"Contact Me"}]